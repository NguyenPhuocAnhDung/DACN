{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b61a24",
   "metadata": {},
   "source": [
    "\n",
    "# **Imports & cấu hình chung**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18057ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # IDS Notebook — Pipeline 2-Phase + Save/Plot + Resume\n",
    "# - Lưu mô hình dạng `.h5` vào D:\\DACN\\results\\training\\models\n",
    "# - Ảnh biểu đồ vào D:\\DACN\\results\\training\\plots\n",
    "# - Bảng so sánh vào D:\\DACN\\results\\training\\tables\n",
    "# - Có thể **tiếp tục train** (resume) mà **không phải chạy lại** tiền xử lý.\n",
    "\n",
    "import os, glob, io, time, json, warnings, joblib, random, pickle, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    precision_recall_fscore_support, accuracy_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==== PATHS (chỉnh theo máy bạn) ====\n",
    "CIC2019_DIR = r'E:\\DACN\\dataset\\CICDDoS2019'\n",
    "CIC2017_DIR = r'E:\\DACN\\dataset\\CICDDoS2017'\n",
    "UNSW15_DIR  = r'E:\\DACN\\dataset\\UNSW_NB15'\n",
    "NSLKDD_DIR  = r'E:\\DACN\\dataset\\NSL-KDD'   # có KDDTrain+.txt, KDDTest+.txt\n",
    "\n",
    "# ==== nơi lưu kết quả ====\n",
    "ROOT_SAVE = Path(r\"E:\\DACN\\results\\training\")\n",
    "DIR_MODELS = ROOT_SAVE / \"models\"\n",
    "DIR_PLOTS  = ROOT_SAVE / \"plots\"\n",
    "DIR_TABLES = ROOT_SAVE / \"tables\"\n",
    "for p in [DIR_MODELS, DIR_PLOTS, DIR_TABLES]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==== loại cột ID/time ====\n",
    "EXCLUDE_ID_COLUMNS = True\n",
    "ID_LIKE_COLS = set([\n",
    "    'Flow ID','FlowID','Timestamp','StartTime','Start Time','stime','time','Date','datetime',\n",
    "    'Src IP','Dst IP','Source IP','Destination IP',\n",
    "    'srcip','dstip','srcip_addr','dstip_addr', \n",
    "    'Src Port','Dst Port','Sport','Dport','srcport','dstport',\n",
    "    'ProtocolName','ProtoName','Service','service','state','attack_cat','label',\n",
    "    'Unnamed: 0','id','No.','Index'\n",
    "])\n",
    "LABEL_CANDS = [\"Label\",\"label\",\"Attack\",\"attack\",\"attack_cat\",\"class\",\"Class\",\"target\",\"category\",\"Category\",\"result\"]\n",
    "\n",
    "# kiểm soát lệch phân bố từ UNSW (toàn attack)\n",
    "MAX_UNSW_RATIO = 0.30   # tối đa 30%\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d557bf",
   "metadata": {},
   "source": [
    "\n",
    "# **Giới hạn CPU + TensorFlow threads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4189767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CPU] total=28 | allow=25 threads (~90%)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import multiprocessing as mp\n",
    "\n",
    "def limit_cpu(fraction: float = 0.90) -> int:\n",
    "    \"\"\"\n",
    "    Giới hạn tài nguyên CPU ~fraction (theo số luồng).\n",
    "    Trả về số threads cho n_jobs/num_threads.\n",
    "    \"\"\"\n",
    "    fraction = max(0.1, min(1.0, float(fraction)))\n",
    "    total = os.cpu_count() or mp.cpu_count() or 1\n",
    "    allow = max(1, math.floor(total * fraction))\n",
    "\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(allow)\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(allow)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(allow)\n",
    "    os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(allow)\n",
    "    os.environ[\"NUMEXPR_MAX_THREADS\"] = str(allow)\n",
    "    os.environ[\"NUMEXPR_NUM_THREADS\"] = str(allow)\n",
    "\n",
    "    try:\n",
    "        from threadpoolctl import threadpool_limits\n",
    "        threadpool_limits(allow)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import psutil\n",
    "        p = psutil.Process()\n",
    "        cpus = list(range(allow))\n",
    "        p.cpu_affinity(cpus)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"[CPU] total={total} | allow={allow} threads (~{fraction*100:.0f}%)\")\n",
    "    return allow\n",
    "\n",
    "threads_allowed = limit_cpu(0.90)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.threading.set_intra_op_parallelism_threads(threads_allowed)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(max(1, threads_allowed//2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734c362",
   "metadata": {},
   "source": [
    "\n",
    "# **Hàm I/O an toàn + Chuẩn hoá nhãn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3749d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_read_any(path: str, nrows=None) -> pd.DataFrame:\n",
    "    low = path.lower()\n",
    "    try:\n",
    "        if low.endswith(\".parquet\"):\n",
    "            return pd.read_parquet(path) if nrows is None else pd.read_parquet(path).head(nrows)\n",
    "        # NSL-KDD .txt không header\n",
    "        if low.endswith(\".txt\") and (\"kddtrain\" in low or \"kddtest\" in low):\n",
    "            df = pd.read_csv(path, header=None)\n",
    "            if df.shape[1] == 43:\n",
    "                cols = [f\"feat_{i}\" for i in range(41)] + [\"label\",\"difficulty\"]\n",
    "            elif df.shape[1] == 42:\n",
    "                cols = [f\"feat_{i}\" for i in range(41)] + [\"label\"]\n",
    "            else:\n",
    "                cols = [f\"col_{i}\" for i in range(df.shape[1])]\n",
    "            df.columns = cols\n",
    "            return df if nrows is None else df.head(nrows)\n",
    "        # CSV chung\n",
    "        for enc in (\"utf-8-sig\",\"utf-8\",\"cp1252\",\"latin1\"):\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc, compression=\"infer\", low_memory=False, nrows=nrows)\n",
    "            except Exception:\n",
    "                continue\n",
    "        return pd.read_csv(path, compression=\"infer\", low_memory=False, nrows=nrows)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] skip {os.path.basename(path)}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def find_label_col(df: pd.DataFrame):\n",
    "    for c in LABEL_CANDS:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "attack_group_map = {\n",
    "    'DrDoS_DNS':'DrDoS','DrDoS_SNMP':'DrDoS','DrDoS_NTP':'DrDoS','DrDoS_MSSQL':'DrDoS',\n",
    "    'DrDoS_SSDP':'DrDoS','DrDoS_UDP':'DrDoS','TFTP':'TFTP',\n",
    "    'UDP':'UDP','UDPLag':'UDP','Syn':'Syn','MSSQL':'MSSQL','LDAP':'LDAP',\n",
    "    'DoS slowloris':'DoS','DoS Slowhttptest':'DoS','DoS Hulk':'DoS','DoS GoldenEye':'DoS',\n",
    "    'Heartbleed':'Other',\n",
    "    'Web Attack � Brute Force':'Web Attack','Web Attack � XSS':'Web Attack','Web Attack � Sql Injection':'Web Attack',\n",
    "    'FTP-Patator':'Brute Force','SSH-Patator':'Brute Force','Infiltration':'Other','Bot':'Other',\n",
    "    'PortScan':'PortScan','NetBIOS':'Other'\n",
    "}\n",
    "\n",
    "def normalize_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    lbl = find_label_col(df)\n",
    "    if lbl is None:\n",
    "        return pd.DataFrame()\n",
    "    df.rename(columns={lbl: \"Label\"}, inplace=True)\n",
    "    df[\"Label\"] = df[\"Label\"].astype(str).str.strip()\n",
    "    df.loc[df[\"Label\"].str.lower().isin([\"normal\",\"benign\",\"non-attack\",\"good\"]), \"Label\"] = \"Benign\"\n",
    "    if \"AttackType\" not in df.columns:\n",
    "        df[\"AttackType\"] = df[\"Label\"]\n",
    "    def group_attack_type(x):\n",
    "        if pd.isna(x): return 'Other'\n",
    "        if x == 'Benign': return 'Benign'\n",
    "        return attack_group_map.get(str(x), 'Other')\n",
    "    df[\"AttackType\"] = df[\"AttackType\"].apply(group_attack_type)\n",
    "    df[\"Label\"] = df[\"Label\"].apply(lambda v: 'Benign' if str(v)=='Benign' else 'DDoS')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b4acd",
   "metadata": {},
   "source": [
    "\n",
    "# **Liệt kê file & union features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471d6fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIC19 train: 7 CIC19 test: 10\n",
      "CIC17: 8 UNSW: 6 NSL: 4\n",
      "Tổng số cột numeric union: 156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['E:\\\\DACN\\\\results\\\\training\\\\feature_order_union.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CIC-2019 train/test\n",
    "cic19_train, cic19_test = [], []\n",
    "for root,_,files in os.walk(CIC2019_DIR):\n",
    "    for fn in files:\n",
    "        if fn.endswith(\"-training.parquet\"): cic19_train.append(os.path.join(root, fn))\n",
    "        if fn.endswith(\"-testing.parquet\"):  cic19_test.append(os.path.join(root, fn))\n",
    "\n",
    "# CIC-2017 parquet\n",
    "cic17_files = glob.glob(os.path.join(CIC2017_DIR, \"**\", \"*.parquet\"), recursive=True)\n",
    "\n",
    "# UNSW: bỏ *_features.csv, *_LIST_EVENTS.csv, *_GT.csv\n",
    "unsw_all = glob.glob(os.path.join(UNSW15_DIR, \"**\", \"*.csv\"), recursive=True)\n",
    "unsw_files = [p for p in unsw_all if (\"features\" not in os.path.basename(p).lower()\n",
    "                                      and \"list_events\" not in os.path.basename(p).lower()\n",
    "                                      and not os.path.basename(p).lower().endswith(\"_gt.csv\"))]\n",
    "\n",
    "# NSL: chỉ .txt\n",
    "nsl_all = glob.glob(os.path.join(NSLKDD_DIR, \"**\", \"*.txt\"), recursive=True)\n",
    "nsl_files = [p for p in nsl_all if (\"kddtrain\" in os.path.basename(p).lower() or\n",
    "                                    \"kddtest\" in os.path.basename(p).lower())]\n",
    "\n",
    "print(\"CIC19 train:\", len(cic19_train), \"CIC19 test:\", len(cic19_test))\n",
    "print(\"CIC17:\", len(cic17_files), \"UNSW:\", len(unsw_files), \"NSL:\", len(nsl_files))\n",
    "\n",
    "def infer_numeric_cols(files: List[str]) -> set:\n",
    "    s = set()\n",
    "    for p in files[:10]:\n",
    "        head = safe_read_any(p, nrows=200)\n",
    "        if head.empty: continue\n",
    "        head = normalize_labels(head)\n",
    "        if head.empty: continue\n",
    "        cols = [c for c in head.columns if c not in ID_LIKE_COLS and c not in (\"Label\",\"AttackType\")]\n",
    "        for c in cols:\n",
    "            if pd.api.types.is_numeric_dtype(head[c]):\n",
    "                s.add(c)\n",
    "    s.add(\"dataset_id\")\n",
    "    return s\n",
    "\n",
    "union_cols = set()\n",
    "union_cols |= infer_numeric_cols(cic19_train + cic19_test)\n",
    "union_cols |= infer_numeric_cols(cic17_files)\n",
    "union_cols |= infer_numeric_cols(unsw_files)\n",
    "union_cols |= infer_numeric_cols(nsl_files)\n",
    "\n",
    "FEATURES = sorted(list(union_cols))\n",
    "print(\"Tổng số cột numeric union:\", len(FEATURES))\n",
    "joblib.dump({\"feature_order\": FEATURES}, ROOT_SAVE / \"feature_order_union.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06df568",
   "metadata": {},
   "source": [
    "\n",
    "# **Load & Normalize datasets + Gộp + Lưu parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947f0b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load ds1: 100%|██████████| 8/8 [00:01<00:00,  4.18it/s]\n",
      "Load ds2: 100%|██████████| 7/7 [00:00<00:00, 41.07it/s]\n",
      "Load ds2: 100%|██████████| 10/10 [00:00<00:00, 28.14it/s]\n",
      "Load ds3: 100%|██████████| 6/6 [00:09<00:00,  1.51s/it]\n",
      "Load ds4: 100%|██████████| 4/4 [00:00<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: {'CIC17': (2313810, 80), 'CIC19_train': (125170, 80), 'UNSW': (257673, 47), 'NSL': (185559, 45)}\n",
      "Đã lưu: E:\\DACN\\results\\training\\df_all_union.parquet\n"
     ]
    }
   ],
   "source": [
    "def load_and_normalize(files: List[str], dataset_id: int) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for p in tqdm(files, desc=f\"Load ds{dataset_id}\"):\n",
    "        df = safe_read_any(p)\n",
    "        if df.empty: \n",
    "            continue\n",
    "        df = normalize_labels(df)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        df[\"dataset_id\"] = dataset_id\n",
    "        out.append(df)\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "df17  = load_and_normalize(cic17_files, 1)\n",
    "df19t = load_and_normalize(cic19_train, 2)\n",
    "df19e = load_and_normalize(cic19_test, 2)\n",
    "dfUN  = load_and_normalize(unsw_files, 3)\n",
    "dfNSL = load_and_normalize(nsl_files, 4)\n",
    "\n",
    "print(\"Shapes:\", {k:v.shape for k,v in {\"CIC17\":df17,\"CIC19_train\":df19t,\"UNSW\":dfUN,\"NSL\":dfNSL}.items()})\n",
    "\n",
    "# gộp chính (2017 + 2019 train)\n",
    "df_main = pd.concat([df17, df19t], ignore_index=True)\n",
    "\n",
    "# hạn chế UNSW (gần như toàn attack)\n",
    "if not dfUN.empty:\n",
    "    cur_ddos = (df_main[\"Label\"]==\"DDoS\").sum()\n",
    "    cap = int(MAX_UNSW_RATIO * max(1, cur_ddos))\n",
    "    dfUN_ddos = dfUN[dfUN[\"Label\"]==\"DDoS\"]\n",
    "    if len(dfUN_ddos) > cap:\n",
    "        dfUN_ddos = dfUN_ddos.sample(cap, random_state=RANDOM_STATE)\n",
    "    dfUN = dfUN_ddos\n",
    "\n",
    "df_all = pd.concat([df_main, dfUN, dfNSL], ignore_index=True)\n",
    "assert not df_all.empty, \"Không có dữ liệu!\"\n",
    "\n",
    "df_all = df_all.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "for c in df_all.columns:\n",
    "    if df_all[c].dtype == \"object\":\n",
    "        df_all[c] = df_all[c].astype(str)\n",
    "\n",
    "parq_path = ROOT_SAVE / \"df_all_union.parquet\"\n",
    "df_all.to_parquet(parq_path, index=False)\n",
    "print(\"Đã lưu:\", parq_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692909f7",
   "metadata": {},
   "source": [
    "\n",
    "# **Đọc lại parquet + Tạo tập train/test + SMOTE + Chuẩn bị DL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e43a354e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đọc lại: (2749109, 166)\n",
      "Số cột dùng: 156\n",
      "Train: (2199287, 156) Test: (549822, 156)\n",
      "After SMOTE: (3386240, 156) | pos_ratio: 0.5\n",
      "DL shapes: (3386240, 156, 1) (549822, 156, 1)\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_parquet(ROOT_SAVE / \"df_all_union.parquet\")\n",
    "print(\"Đọc lại:\", df_all.shape)\n",
    "\n",
    "drop_cols = {'Label','AttackType'}\n",
    "if EXCLUDE_ID_COLUMNS:\n",
    "    drop_cols |= {c for c in df_all.columns if c in ID_LIKE_COLS}\n",
    "feature_candidates = [c for c in FEATURES if c not in drop_cols and c in df_all.columns]\n",
    "print(\"Số cột dùng:\", len(feature_candidates))\n",
    "\n",
    "X = df_all.reindex(columns=feature_candidates, fill_value=0.0).astype(np.float32)\n",
    "y_bin = (df_all['Label'] != 'Benign').astype(int).values\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y_bin, test_size=0.2, random_state=RANDOM_STATE, stratify=y_bin\n",
    ")\n",
    "print(\"Train:\", X_train_raw.shape, \"Test:\", X_test_raw.shape)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_raw.values)\n",
    "joblib.dump(scaler, ROOT_SAVE / 'scaler_union.pkl')\n",
    "\n",
    "X_train_s = scaler.transform(X_train_raw.values)\n",
    "X_test_s  = scaler.transform(X_test_raw.values)\n",
    "\n",
    "sm = SMOTE(random_state=RANDOM_STATE)\n",
    "X_res, y_res = sm.fit_resample(X_train_s, y_train)\n",
    "print(\"After SMOTE:\", X_res.shape, \"| pos_ratio:\", y_res.mean().round(4))\n",
    "\n",
    "# Dữ liệu cho DL (mỗi feature là 1 “time step”)\n",
    "X_train_dl = X_res.astype(np.float32).reshape(-1, X_res.shape[1], 1)\n",
    "X_test_dl  = X_test_s.astype(np.float32).reshape(-1, X_test_s.shape[1], 1)\n",
    "y_train_dl = y_res.astype(np.int32)\n",
    "y_test_dl  = y_test.astype(np.int32)\n",
    "\n",
    "print(\"DL shapes:\", X_train_dl.shape, X_test_dl.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d36b79f",
   "metadata": {},
   "source": [
    "\n",
    "# **Tiện ích Save/Load .h5 + Plot (CM + Val)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec3b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def _stamp():\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def save_fig_current(fig, name: str):\n",
    "    png = DIR_PLOTS / f\"{name}-{_stamp()}.png\"\n",
    "    fig.savefig(png, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"[SAVE] Figure -> {png}\")\n",
    "    return str(png)\n",
    "\n",
    "def save_model_h5_any(model, name: str, extra: dict | None = None):\n",
    "    \"\"\"\n",
    "    Lưu mô hình dạng .h5:\n",
    "    - Keras: model.save(.h5)\n",
    "    - Sklearn/XGBoost/LightGBM: pickle vào HDF5['pickle'] + attrs['extra_json']\n",
    "    \"\"\"\n",
    "    path = DIR_MODELS / f\"{name}-{_stamp()}.h5\"\n",
    "    # Thử Keras trước\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        if hasattr(model, \"save\") and isinstance(getattr(model, \"save\"), type(tf.keras.Model.save)):\n",
    "            model.save(path)\n",
    "            print(f\"[SAVE] Keras model -> {path}\")\n",
    "            if extra:\n",
    "                with open(str(path).replace(\".h5\", \".meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(extra, f, ensure_ascii=False, indent=2, default=str)\n",
    "            return str(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Non-Keras\n",
    "    blob = pickle.dumps(model, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with h5py.File(path, \"w\") as h5:\n",
    "        h5.create_dataset(\"pickle\", data=np.void(blob))\n",
    "        if extra:\n",
    "            try:\n",
    "                h5.attrs[\"extra_json\"] = json.dumps(extra, default=str)\n",
    "            except Exception:\n",
    "                h5.attrs[\"extra_json\"] = \"{}\"\n",
    "    print(f\"[SAVE] Pickled model-in-HDF5 -> {path}\")\n",
    "    return str(path)\n",
    "\n",
    "def load_model_h5_any(path: str):\n",
    "    import tensorflow as tf\n",
    "    try:\n",
    "        return tf.keras.models.load_model(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    with h5py.File(path, \"r\") as h5:\n",
    "        blob = bytes(h5[\"pickle\"][()])\n",
    "    return pickle.loads(blob)\n",
    "\n",
    "# --- Plot: Binary\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "def show_cm_and_valacc(model_name, y_true, y_prob, threshold=0.5, savepath=None):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    cm  = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "    im = axes[0].imshow(cm, cmap=\"Blues\")\n",
    "    for (i,j),v in np.ndenumerate(cm):\n",
    "        axes[0].text(j, i, str(v), ha=\"center\", va=\"center\", fontsize=10)\n",
    "    axes[0].set_xticks([0,1]); axes[0].set_xticklabels([\"Benign\",\"DDoS\"])\n",
    "    axes[0].set_yticks([0,1]); axes[0].set_yticklabels([\"Benign\",\"DDoS\"])\n",
    "    axes[0].set_xlabel(\"Predicted\"); axes[0].set_ylabel(\"Actual\")\n",
    "    axes[0].set_title(f\"{model_name} — Confusion Matrix\")\n",
    "    fig.colorbar(im, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    axes[1].bar([0], [acc], width=0.5)\n",
    "    axes[1].set_ylim(0, 1.0)\n",
    "    axes[1].set_xticks([0]); axes[1].set_xticklabels([\"Accuracy\"])\n",
    "    axes[1].set_ylabel(\"Value\")\n",
    "    axes[1].set_title(f\"{model_name} — Validation Accuracy\")\n",
    "    axes[1].text(0, min(acc+0.03, 0.98), f\"{acc:.6f}\", ha=\"center\", va=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "    plt.suptitle(f\"{model_name}  |  ACC={acc:.6f}  AUC={auc:.6f}  thr={threshold}\", y=1.04, fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    if savepath: plt.savefig(savepath, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Plot: Multiclass\n",
    "def show_cm_and_valacc_multiclass(model_name, y_true, y_pred, labels, savepath=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(labels)))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    im = axes[0].imshow(cm_norm, cmap=\"Blues\", vmin=0, vmax=1)\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        axes[0].text(j, i, str(v), ha=\"center\", va=\"center\", fontsize=8,\n",
    "                     color=\"white\" if cm_norm[i, j] > 0.5 else \"black\")\n",
    "    axes[0].set_xticks(np.arange(len(labels))); axes[0].set_xticklabels(labels, rotation=90)\n",
    "    axes[0].set_yticks(np.arange(len(labels))); axes[0].set_yticklabels(labels)\n",
    "    axes[0].set_xlabel(\"Predicted\"); axes[0].set_ylabel(\"Actual\")\n",
    "    axes[0].set_title(f\"{model_name} — Confusion Matrix (Normalized)\")\n",
    "    fig.colorbar(im, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    axes[1].bar([0], [acc], width=0.5)\n",
    "    axes[1].set_ylim(0, 1.0)\n",
    "    axes[1].set_xticks([0]); axes[1].set_xticklabels([\"Accuracy\"])\n",
    "    axes[1].set_ylabel(\"Value\")\n",
    "    axes[1].set_title(f\"{model_name} — Validation Accuracy\")\n",
    "    axes[1].text(0, min(acc+0.03, 0.98), f\"{acc:.4f}\", ha=\"center\", va=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "    plt.suptitle(f\"{model_name}  |  ACC={acc:.4f}\", y=1.03, fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"[Saved] {savepath}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7d03c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EarlyStop theo \"cải thiện quá nhỏ\" giữa 2 epoch liên tiếp (Keras) =====\n",
    "from tensorflow.keras import layers, models, callbacks  # type: ignore\n",
    "\n",
    "class StopOnTinyChange(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Dừng sớm nếu metric theo dõi (monitor) cải thiện quá nhỏ giữa 2 epoch liên tiếp.\n",
    "    - monitor: 'val_auc' cho bài toán binary, hoặc 'val_accuracy' cho multiclass.\n",
    "    - min_delta: ngưỡng cải thiện tối thiểu. Nếu |Δ| < min_delta => dừng.\n",
    "    \"\"\"\n",
    "    def __init__(self, monitor=\"val_auc\", min_delta=1e-4):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.min_delta = float(min_delta)\n",
    "        self.prev = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        curr = logs.get(self.monitor)\n",
    "        if curr is None:\n",
    "            return\n",
    "        if self.prev is not None and abs(curr - self.prev) < self.min_delta:\n",
    "            print(f\"\\n[STOP] Δ{self.monitor}={curr - self.prev:.6f} < {self.min_delta} tại epoch {epoch+1}. Kết thúc huấn luyện.\")\n",
    "            self.model.stop_training = True\n",
    "        self.prev = curr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46eea880",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_BIN = []  # lưu so sánh phase-1\n",
    "\n",
    "def eval_binary(y_true, y_prob, name, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    RESULTS_BIN.append({\"Model\": name, \"ACC\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1, \"AUC\": auc})\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Benign\",\"DDoS\"]))\n",
    "    print(\"ROC-AUC:\", auc)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a880750",
   "metadata": {},
   "source": [
    "\n",
    "# **Phase 1: Deep Learning (Binary — LSTM/GRU/CNN) — Train/Resume + Lưu + Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Epoch 1/3 — best_iter: 542\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks # type: ignore\n",
    "\n",
    "EPOCHS_DL = 20\n",
    "BATCH = 2048\n",
    "\n",
    "cb = [\n",
    "    callbacks.EarlyStopping(patience=3, restore_best_weights=True, monitor=\"val_auc\", mode=\"max\"),\n",
    "    callbacks.ReduceLROnPlateau(patience=2, factor=0.5, min_lr=1e-5, monitor=\"val_auc\", mode=\"max\"),\n",
    "    StopOnTinyChange(monitor=\"val_auc\", min_delta=1e-4),\n",
    "]\n",
    "\n",
    "def compile_binary(model):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[tf.keras.metrics.AUC(name=\"auc\")])\n",
    "    return model\n",
    "\n",
    "# ----- LSTM -----\n",
    "if 'lstm' not in globals():\n",
    "    lstm = models.Sequential([\n",
    "        layers.Input(shape=(X_train_dl.shape[1], 1)),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.LSTM(32),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "compile_binary(lstm)\n",
    "lstm.fit(\n",
    "    X_train_dl, y_train_dl,\n",
    "    epochs=EPOCHS_DL, batch_size=BATCH,\n",
    "    validation_data=(X_test_dl, y_test_dl),\n",
    "    callbacks=cb,\n",
    "    verbose=1                                      # <<< BẬT LOG CHI TIẾT\n",
    ")\n",
    "yprob_lstm = lstm.predict(X_test_dl, batch_size=BATCH, verbose=0).ravel()\n",
    "_ = eval_binary(y_test, yprob_lstm, \"LSTM (Binary)\")\n",
    "save_model_h5_any(lstm, \"LSTM_Binary_Phase1\")\n",
    "show_cm_and_valacc(\"LSTM (Binary)\", y_test, yprob_lstm, threshold=0.5,\n",
    "                   savepath=str(DIR_PLOTS / f\"LSTM_Binary_Phase1-{_stamp()}.png\"))\n",
    "\n",
    "# ----- GRU -----\n",
    "if 'gru' not in globals():\n",
    "    gru = models.Sequential([\n",
    "        layers.Input(shape=(X_train_dl.shape[1], 1)),\n",
    "        layers.GRU(64, return_sequences=True),\n",
    "        layers.GRU(32),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "compile_binary(gru)\n",
    "gru.fit(X_train_dl, y_train_dl, epochs=EPOCHS_DL, batch_size=BATCH,\n",
    "        validation_data=(X_test_dl, y_test_dl), callbacks=cb, verbose=0)\n",
    "yprob_gru = gru.predict(X_test_dl, batch_size=BATCH, verbose=0).ravel()\n",
    "_ = eval_binary(y_test, yprob_gru, \"GRU (Binary)\")\n",
    "save_model_h5_any(gru, \"GRU_Binary_Phase1\")\n",
    "show_cm_and_valacc(\"GRU (Binary)\", y_test, yprob_gru, threshold=0.5,\n",
    "                   savepath=str(DIR_PLOTS / f\"GRU_Binary_Phase1-{_stamp()}.png\"))\n",
    "\n",
    "# ----- 1D-CNN -----\n",
    "if 'cnn' not in globals():\n",
    "    cnn = models.Sequential([\n",
    "        layers.Input(shape=(X_train_dl.shape[1], 1)),\n",
    "        layers.Conv1D(64, kernel_size=5, activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=3, activation=\"relu\"),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "compile_binary(cnn)\n",
    "cnn.fit(X_train_dl, y_train_dl, epochs=EPOCHS_DL, batch_size=BATCH,\n",
    "        validation_data=(X_test_dl, y_test_dl), callbacks=cb, verbose=0)\n",
    "yprob_cnn = cnn.predict(X_test_dl, batch_size=BATCH, verbose=0).ravel()\n",
    "_ = eval_binary(y_test, yprob_cnn, \"1D-CNN (Binary)\")\n",
    "save_model_h5_any(cnn, \"CNN1D_Binary_Phase1\")\n",
    "show_cm_and_valacc(\"1D-CNN (Binary)\", y_test, yprob_cnn, threshold=0.5,\n",
    "                   savepath=str(DIR_PLOTS / f\"CNN1D_Binary_Phase1-{_stamp()}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a00b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mới thêm có thể xóa (thêm để test độ hiệu quả)\n",
    "# %% DL (Binary) — nâng chất & tối ưu ngưỡng cho LSTM/GRU/CNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks  # type: ignore\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "\n",
    "# EPOCHS_DL = 25         # 20 -> 25 (vẫn early stop)\n",
    "# BATCH = 2048\n",
    "EPOCHS_DL = 3\n",
    "BATCH = 4096\n",
    "\n",
    "# ----- Callbacks giữ nguyên -----\n",
    "cb = [\n",
    "    callbacks.EarlyStopping(patience=4, restore_best_weights=True, monitor=\"val_auc\", mode=\"max\"),\n",
    "    callbacks.ReduceLROnPlateau(patience=2, factor=0.5, min_lr=1e-5, monitor=\"val_auc\", mode=\"max\"),\n",
    "    StopOnTinyChange(monitor=\"val_auc\", min_delta=1e-4),\n",
    "]\n",
    "\n",
    "# ===== Focal loss & tối ưu ngưỡng =====\n",
    "def focal_binary_loss(gamma=2.0, alpha=0.25):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        eps = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, eps, 1.0-eps)\n",
    "        pt = tf.where(tf.equal(y_true,1), y_pred, 1-y_pred)\n",
    "        w  = tf.where(tf.equal(y_true,1), alpha, 1-alpha)\n",
    "        return -tf.reduce_mean(w * tf.pow(1-pt, gamma) * tf.math.log(pt))\n",
    "    return loss\n",
    "\n",
    "def find_best_threshold(y_true, y_prob, min_recall=None):\n",
    "    p, r, thr = precision_recall_curve(y_true, y_prob)\n",
    "    f1 = 2*p[:-1]*r[:-1]/(p[:-1]+r[:-1] + 1e-9)\n",
    "    if min_recall is not None:\n",
    "        mask = r[:-1] >= float(min_recall)\n",
    "        if mask.any():\n",
    "            idx = np.flatnonzero(mask)[f1[mask].argmax()]\n",
    "        else:\n",
    "            idx = f1.argmax()\n",
    "    else:\n",
    "        idx = f1.argmax()\n",
    "    return float(thr[idx]), float(p[idx]), float(r[idx]), float(f1[idx])\n",
    "\n",
    "# ===== compile tiện dụng (focal + jit) =====\n",
    "def compile_binary(model, use_focal=True, lr=1e-3):\n",
    "    if use_focal:\n",
    "        loss_fn = focal_binary_loss(gamma=2.0, alpha=0.25)\n",
    "    else:\n",
    "        loss_fn = \"binary_crossentropy\"\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr),\n",
    "        loss=loss_fn,\n",
    "        metrics=[tf.keras.metrics.AUC(name=\"auc\")],\n",
    "        jit_compile=True  # tăng tốc TF2.16\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# LSTM — nâng cấp nhẹ\n",
    "# =========================\n",
    "if 'lstm' not in globals():\n",
    "    lstm = models.Sequential([\n",
    "        layers.Input(shape=(X_train_dl.shape[1], 1)),\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True,\n",
    "                                         dropout=0.2, recurrent_dropout=0.2)),\n",
    "        layers.Bidirectional(layers.LSTM(64, return_sequences=False,\n",
    "                                         dropout=0.2, recurrent_dropout=0.2)),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "compile_binary(lstm, use_focal=True, lr=1e-3)\n",
    "lstm.fit(X_train_dl, y_train_dl, epochs=EPOCHS_DL, batch_size=BATCH,\n",
    "         validation_data=(X_test_dl, y_test_dl), callbacks=cb, verbose=1)\n",
    "\n",
    "yprob_lstm = lstm.predict(X_test_dl, batch_size=BATCH, verbose=0).ravel()\n",
    "best_thr_lstm, bp, br, bf1 = find_best_threshold(y_test, yprob_lstm, min_recall=0.85)  # đặt None nếu chỉ tối đa F1\n",
    "print(f\"[LSTM] Best thr={best_thr_lstm:.4f} | P={bp:.3f} R={br:.3f} F1={bf1:.3f}\")\n",
    "print(classification_report(y_test, (yprob_lstm >= best_thr_lstm).astype(int), target_names=['Benign','DDoS']))\n",
    "\n",
    "# log vào bảng tổng hợp Phase-1 với ngưỡng tối ưu\n",
    "_ = eval_binary(y_test, yprob_lstm, f\"LSTM (Binary, thr@{best_thr_lstm:.3f})\", threshold=best_thr_lstm)\n",
    "save_model_h5_any(lstm, \"LSTM_Binary_Phase1\", extra={\"best_threshold\": best_thr_lstm})\n",
    "show_cm_and_valacc(\"LSTM (Binary)\", y_test, yprob_lstm, threshold=best_thr_lstm,\n",
    "                   savepath=str(DIR_PLOTS / f\"LSTM_Binary_Phase1-{_stamp()}.png\"))\n",
    "\n",
    "# =========================\n",
    "# GRU — giữ kiến trúc, thêm focal + jit + chọn ngưỡng\n",
    "# =========================\n",
    "if 'gru' not in globals():\n",
    "    gru = models.Sequential([\n",
    "        layers.Input(shape=(X_train_dl.shape[1], 1)),\n",
    "        layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "        layers.GRU(64,  return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "compile_binary(gru, use_focal=True, lr=1e-3)\n",
    "gru.fit(X_train_dl, y_train_dl, epochs=EPOCHS_DL, batch_size=BATCH,\n",
    "        validation_data=(X_test_dl, y_test_dl), callbacks=cb, verbose=1)\n",
    "\n",
    "yprob_gru = gru.predict(X_test_dl, batch_size=BATCH, verbose=0).ravel()\n",
    "best_thr_gru, bp, br, bf1 = find_best_threshold(y_test, yprob_gru, min_recall=0.85)\n",
    "print(f\"[GRU] Best thr={best_thr_gru:.4f} | P={bp:.3f} R={br:.3f} F1={bf1:.3f}\")\n",
    "_ = eval_binary(y_test, yprob_gru, f\"GRU (Binary, thr@{best_thr_gru:.3f})\", threshold=best_thr_gru)\n",
    "save_model_h5_any(gru, \"GRU_Binary_Phase1\", extra={\"best_threshold\": best_thr_gru})\n",
    "show_cm_and_valacc(\"GRU (Binary)\", y_test, yprob_gru, threshold=best_thr_gru,\n",
    "                   savepath=str(DIR_PLOTS / f\"GRU_Binary_Phase1-{_stamp()}.png\"))\n",
    "\n",
    "# =========================\n",
    "# 1D-CNN — giữ kiến trúc, thêm focal + jit + chọn ngưỡng\n",
    "# =========================\n",
    "if 'cnn' not in globals():\n",
    "    cnn = models.Sequential([\n",
    "        layers.Input(shape=(X_train_dl.shape[1], 1)),\n",
    "        layers.Conv1D(64, kernel_size=5, activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=3, activation=\"relu\"),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "compile_binary(cnn, use_focal=True, lr=1e-3)\n",
    "cnn.fit(X_train_dl, y_train_dl, epochs=EPOCHS_DL, batch_size=BATCH,\n",
    "        validation_data=(X_test_dl, y_test_dl), callbacks=cb, verbose=1)\n",
    "\n",
    "yprob_cnn = cnn.predict(X_test_dl, batch_size=BATCH, verbose=0).ravel()\n",
    "best_thr_cnn, bp, br, bf1 = find_best_threshold(y_test, yprob_cnn, min_recall=0.85)\n",
    "print(f\"[CNN] Best thr={best_thr_cnn:.4f} | P={bp:.3f} R={br:.3f} F1={bf1:.3f}\")\n",
    "_ = eval_binary(y_test, yprob_cnn, f\"1D-CNN (Binary, thr@{best_thr_cnn:.3f})\", threshold=best_thr_cnn)\n",
    "save_model_h5_any(cnn, \"CNN1D_Binary_Phase1\", extra={\"best_threshold\": best_thr_cnn})\n",
    "show_cm_and_valacc(\"1D-CNN (Binary)\", y_test, yprob_cnn, threshold=best_thr_cnn,\n",
    "                   savepath=str(DIR_PLOTS / f\"CNN1D_Binary_Phase1-{_stamp()}.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81be7f",
   "metadata": {},
   "source": [
    "\n",
    "# **Phase 2: Chuẩn bị dữ liệu đa lớp (AttackType)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy tất cả mẫu DDoS\n",
    "df_attack = df_all[df_all['Label']=='DDoS'].copy()\n",
    "X_attack = df_attack.reindex(columns=feature_candidates, fill_value=0.0).astype(np.float32)\n",
    "X_attack_s = scaler.transform(X_attack)\n",
    "\n",
    "# Mã hoá nhãn AttackType\n",
    "y_attack_txt = df_attack['AttackType'].astype(str).values\n",
    "le_attack = LabelEncoder()\n",
    "y_attack = le_attack.fit_transform(y_attack_txt)\n",
    "num_classes = len(le_attack.classes_)\n",
    "joblib.dump(le_attack, ROOT_SAVE / \"attack_label_encoder_union.pkl\")\n",
    "print(\"Classes:\", list(le_attack.classes_))\n",
    "\n",
    "# SMOTE multiclass\n",
    "X_attack_res, y_attack_res = SMOTE(random_state=RANDOM_STATE).fit_resample(X_attack_s, y_attack)\n",
    "\n",
    "# Train/test split\n",
    "Xa_tr, Xa_te, ya_tr, ya_te = train_test_split(\n",
    "    X_attack_res, y_attack_res, test_size=0.2, random_state=RANDOM_STATE, stratify=y_attack_res\n",
    ")\n",
    "Xa_tr = Xa_tr.astype(np.float16)\n",
    "Xa_te = Xa_te.astype(np.float16)\n",
    "print(\"Train:\", Xa_tr.shape, \"Test:\", Xa_te.shape)\n",
    "\n",
    "# Lưu để DL Phase-2 dùng\n",
    "Xa_tr_dl = Xa_tr.astype(np.float32).reshape(-1, Xa_tr.shape[1], 1)\n",
    "Xa_te_dl = Xa_te.astype(np.float32).reshape(-1, Xa_te.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a52489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase 2 ở cell trên (thử nghiệm)\n",
    "# %% DL (Multiclass) — LSTM/GRU/CNN cho AttackType (Phase-2)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks  # type: ignore\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "EPOCHS_DL2 = 5\n",
    "BATCH2 = 4096\n",
    "num_classes = len(le_attack.classes_)\n",
    "input_len = Xa_tr_dl.shape[1]\n",
    "\n",
    "cb_mc = [\n",
    "    callbacks.EarlyStopping(patience=4, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
    "    callbacks.ReduceLROnPlateau(patience=2, factor=0.5, min_lr=1e-5),\n",
    "    StopOnTinyChange(monitor=\"val_accuracy\", min_delta=1e-4),\n",
    "]\n",
    "\n",
    "# ---------- LSTM (Multiclass) ----------\n",
    "if 'lstm_mc' not in globals():\n",
    "    lstm_mc = models.Sequential([\n",
    "        layers.Input(shape=(input_len, 1)),\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True,\n",
    "                                         dropout=0.2, recurrent_dropout=0.2)),\n",
    "        layers.Bidirectional(layers.LSTM(64, return_sequences=False,\n",
    "                                         dropout=0.2, recurrent_dropout=0.2)),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "lstm_mc.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                loss=\"sparse_categorical_crossentropy\",\n",
    "                metrics=[\"accuracy\"],\n",
    "                jit_compile=True)\n",
    "lstm_mc.fit(Xa_tr_dl, ya_tr, epochs=EPOCHS_DL2, batch_size=BATCH2,\n",
    "            validation_data=(Xa_te_dl, ya_te), callbacks=cb_mc, verbose=1)\n",
    "y_pred_lstm_mc = tf.argmax(lstm_mc.predict(Xa_te_dl, batch_size=BATCH2, verbose=0), axis=1).numpy()\n",
    "print(\"\\nLSTM (Multiclass) — ACC:\", accuracy_score(ya_te, y_pred_lstm_mc))\n",
    "print(classification_report(ya_te, y_pred_lstm_mc, target_names=le_attack.classes_))\n",
    "save_model_h5_any(lstm_mc, \"LSTM_Multiclass_Phase2\",\n",
    "                  extra={\"classes\": list(le_attack.classes_), \"feature_order\": feature_candidates})\n",
    "show_cm_and_valacc_multiclass(\"LSTM (Multiclass AttackType)\",\n",
    "                              ya_te, y_pred_lstm_mc, labels=le_attack.classes_,\n",
    "                              savepath=str(DIR_PLOTS / f\"LSTM_Multiclass_Phase2-{_stamp()}.png\"))\n",
    "\n",
    "# ---------- GRU (Multiclass) ----------\n",
    "if 'gru_mc' not in globals():\n",
    "    gru_mc = models.Sequential([\n",
    "        layers.Input(shape=(input_len, 1)),\n",
    "        layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "        layers.GRU(64,  return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "gru_mc.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "               loss=\"sparse_categorical_crossentropy\",\n",
    "               metrics=[\"accuracy\"],\n",
    "               jit_compile=True)\n",
    "gru_mc.fit(Xa_tr_dl, ya_tr, epochs=EPOCHS_DL2, batch_size=BATCH2,\n",
    "           validation_data=(Xa_te_dl, ya_te), callbacks=cb_mc, verbose=1)\n",
    "y_pred_gru_mc = tf.argmax(gru_mc.predict(Xa_te_dl, batch_size=BATCH2, verbose=0), axis=1).numpy()\n",
    "print(\"\\nGRU (Multiclass) — ACC:\", accuracy_score(ya_te, y_pred_gru_mc))\n",
    "print(classification_report(ya_te, y_pred_gru_mc, target_names=le_attack.classes_))\n",
    "save_model_h5_any(gru_mc, \"GRU_Multiclass_Phase2\",\n",
    "                  extra={\"classes\": list(le_attack.classes_), \"feature_order\": feature_candidates})\n",
    "show_cm_and_valacc_multiclass(\"GRU (Multiclass AttackType)\",\n",
    "                              ya_te, y_pred_gru_mc, labels=le_attack.classes_,\n",
    "                              savepath=str(DIR_PLOTS / f\"GRU_Multiclass_Phase2-{_stamp()}.png\"))\n",
    "\n",
    "# ---------- 1D-CNN (Multiclass) ----------\n",
    "if 'cnn_mc' not in globals():\n",
    "    cnn_mc = models.Sequential([\n",
    "        layers.Input(shape=(input_len, 1)),\n",
    "        layers.Conv1D(64, kernel_size=5, activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=3, activation=\"relu\"),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "cnn_mc.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "               loss=\"sparse_categorical_crossentropy\",\n",
    "               metrics=[\"accuracy\"],\n",
    "               jit_compile=True)\n",
    "cnn_mc.fit(Xa_tr_dl, ya_tr, epochs=EPOCHS_DL2, batch_size=BATCH2,\n",
    "           validation_data=(Xa_te_dl, ya_te), callbacks=cb_mc, verbose=1)\n",
    "y_pred_cnn_mc = tf.argmax(cnn_mc.predict(Xa_te_dl, batch_size=BATCH2, verbose=0), axis=1).numpy()\n",
    "print(\"\\n1D-CNN (Multiclass) — ACC:\", accuracy_score(ya_te, y_pred_cnn_mc))\n",
    "print(classification_report(ya_te, y_pred_cnn_mc, target_names=le_attack.classes_))\n",
    "save_model_h5_any(cnn_mc, \"CNN1D_Multiclass_Phase2\",\n",
    "                  extra={\"classes\": list(le_attack.classes_), \"feature_order\": feature_candidates})\n",
    "show_cm_and_valacc_multiclass(\"1D-CNN (Multiclass AttackType)\",\n",
    "                              ya_te, y_pred_cnn_mc, labels=le_attack.classes_,\n",
    "                              savepath=str(DIR_PLOTS / f\"CNN1D_Multiclass_Phase2-{_stamp()}.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfceed46",
   "metadata": {},
   "source": [
    "\n",
    "# **Phase 2: Deep Learning (Multiclass — LSTM/GRU/CNN) — Train/Resume + Lưu + Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8409f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks # type: ignore\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\n=== Phase 2 — Deep Learning (Multiclass AttackType) ===\")\n",
    "EPOCHS_DL2 = 25\n",
    "BATCH2 = 2048\n",
    "num_classes = len(le_attack.classes_)\n",
    "print(\"Attack classes:\", list(le_attack.classes_))\n",
    "\n",
    "cb2 = [\n",
    "    callbacks.EarlyStopping(patience=3, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
    "    callbacks.ReduceLROnPlateau(patience=2, factor=0.5, min_lr=1e-5)\n",
    "]\n",
    "\n",
    "# LSTM\n",
    "if 'lstm_multi' not in globals():\n",
    "    lstm_multi = models.Sequential([\n",
    "        layers.Input(shape=(Xa_tr.shape[1], 1)),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.LSTM(32),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "lstm_multi.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(\"\\n[Train] LSTM (Multiclass)\")\n",
    "lstm_multi.fit(Xa_tr_dl, ya_tr, epochs=EPOCHS_DL2, batch_size=BATCH2,\n",
    "               validation_data=(Xa_te_dl, ya_te), callbacks=cb2, verbose=1)\n",
    "y_pred_lstm = np.argmax(lstm_multi.predict(Xa_te_dl, batch_size=BATCH2, verbose=0), axis=1)\n",
    "print(\"LSTM (Multiclass) — Accuracy:\", accuracy_score(ya_te, y_pred_lstm))\n",
    "print(classification_report(ya_te, y_pred_lstm, target_names=le_attack.classes_))\n",
    "save_model_h5_any(lstm_multi, \"LSTM_Multiclass_Phase2\", extra={\"classes\": list(le_attack.classes_)})\n",
    "show_cm_and_valacc_multiclass(\"LSTM (Multiclass AttackType)\", ya_te, y_pred_lstm, labels=le_attack.classes_,\n",
    "                              savepath=str(DIR_PLOTS / f\"LSTM_Multiclass_Phase2-{_stamp()}.png\"))\n",
    "\n",
    "# GRU\n",
    "if 'gru_multi' not in globals():\n",
    "    gru_multi = models.Sequential([\n",
    "        layers.Input(shape=(Xa_tr.shape[1], 1)),\n",
    "        layers.GRU(64, return_sequences=True),\n",
    "        layers.GRU(32),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "gru_multi.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(\"\\n[Train] GRU (Multiclass)\")\n",
    "gru_multi.fit(Xa_tr_dl, ya_tr, epochs=EPOCHS_DL2, batch_size=BATCH2,\n",
    "              validation_data=(Xa_te_dl, ya_te), callbacks=cb2, verbose=1)\n",
    "y_pred_gru = np.argmax(gru_multi.predict(Xa_te_dl, batch_size=BATCH2, verbose=0), axis=1)\n",
    "print(\"GRU (Multiclass) — Accuracy:\", accuracy_score(ya_te, y_pred_gru))\n",
    "print(classification_report(ya_te, y_pred_gru, target_names=le_attack.classes_))\n",
    "save_model_h5_any(gru_multi, \"GRU_Multiclass_Phase2\", extra={\"classes\": list(le_attack.classes_)})\n",
    "show_cm_and_valacc_multiclass(\"GRU (Multiclass AttackType)\", ya_te, y_pred_gru, labels=le_attack.classes_,\n",
    "                              savepath=str(DIR_PLOTS / f\"GRU_Multiclass_Phase2-{_stamp()}.png\"))\n",
    "\n",
    "# 1D-CNN\n",
    "if 'cnn_multi' not in globals():\n",
    "    cnn_multi = models.Sequential([\n",
    "        layers.Input(shape=(Xa_tr.shape[1], 1)),\n",
    "        layers.Conv1D(64, kernel_size=5, activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=3, activation=\"relu\"),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "cnn_multi.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(\"\\n[Train] 1D-CNN (Multiclass)\")\n",
    "cnn_multi.fit(Xa_tr_dl, ya_tr, epochs=EPOCHS_DL2, batch_size=BATCH2,\n",
    "              validation_data=(Xa_te_dl, ya_te), callbacks=cb2, verbose=1)\n",
    "y_pred_cnn = np.argmax(cnn_multi.predict(Xa_te_dl, batch_size=BATCH2, verbose=0), axis=1)\n",
    "print(\"1D-CNN (Multiclass) — Accuracy:\", accuracy_score(ya_te, y_pred_cnn))\n",
    "print(classification_report(ya_te, y_pred_cnn, target_names=le_attack.classes_))\n",
    "save_model_h5_any(cnn_multi, \"CNN1D_Multiclass_Phase2\", extra={\"classes\": list(le_attack.classes_)})\n",
    "show_cm_and_valacc_multiclass(\"1D-CNN (Multiclass AttackType)\", ya_te, y_pred_cnn, labels=le_attack.classes_,\n",
    "                              savepath=str(DIR_PLOTS / f\"CNN1D_Multiclass_Phase2-{_stamp()}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cbc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Phase-2 (Optimized) - Multiclass: LSTM / GRU / 1D-CNN / Hybrid + Ensemble =====\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# reproducible-ish\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n=== Phase 2 — Deep Learning (Multiclass AttackType) — OPTIMIZED ===\")\n",
    "EPOCHS_DL2 = 25\n",
    "BATCH2 = 512                    # giảm batch size để gradient ổn định\n",
    "NUM_CLASSES = len(le_attack.classes_)\n",
    "print(\"Attack classes:\", list(le_attack.classes_))\n",
    "\n",
    "# ----- 1) Preprocessing: scaling + reshape -----\n",
    "scaler = StandardScaler()\n",
    "# Xa_tr, Xa_te assumed shape (n_samples, n_features)\n",
    "Xa_tr_s = scaler.fit_transform(Xa_tr)\n",
    "Xa_te_s = scaler.transform(Xa_te)\n",
    "Xa_tr_dl = Xa_tr_s[..., np.newaxis]   # (N, T, 1)\n",
    "Xa_te_dl = Xa_te_s[..., np.newaxis]\n",
    "\n",
    "# ----- 2) class weights -----\n",
    "cw_vals = compute_class_weight('balanced', classes=np.unique(ya_tr), y=ya_tr)\n",
    "class_weights = dict(enumerate(cw_vals))\n",
    "print(\"[INFO] class_weights sample:\", list(class_weights.items())[:4])\n",
    "\n",
    "# ----- 3) Callbacks -----\n",
    "stamp = _stamp()\n",
    "ckpt_base = f\"Phase2_best-{{name}}-{stamp}.h5\"  # will fill name when saving manually\n",
    "common_cbs = [\n",
    "    callbacks.ModelCheckpoint(filepath=os.path.join(str(DIR_PLOTS), ckpt_base.format(name=\"tmp\")),\n",
    "                             monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.5, patience=2, min_lr=1e-6, verbose=1),\n",
    "    callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "# helper to build a fresh set of callbacks with the right filename\n",
    "def make_cbs(name):\n",
    "    path = os.path.join(str(DIR_PLOTS), ckpt_base.format(name=name))\n",
    "    return [\n",
    "        callbacks.ModelCheckpoint(filepath=path, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
    "        callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.5, patience=2, min_lr=1e-6, verbose=1),\n",
    "        callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True, verbose=1)\n",
    "    ]\n",
    "\n",
    "# ----- 4) Models definitions (improved) -----\n",
    "\n",
    "# LSTM (improved: dropout + layer norm)\n",
    "def build_lstm(name=\"LSTM\"):\n",
    "    inp = layers.Input(shape=(Xa_tr.shape[1], 1))\n",
    "    x = layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.15)(inp)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.LSTM(64, dropout=0.3, recurrent_dropout=0.15)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    m = models.Model(inputs=inp, outputs=out, name=name)\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "# GRU (improved: larger units + dropout)\n",
    "def build_gru(name=\"GRU\"):\n",
    "    inp = layers.Input(shape=(Xa_tr.shape[1], 1))\n",
    "    x = layers.GRU(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.15)(inp)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.GRU(64, dropout=0.3, recurrent_dropout=0.15)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    m = models.Model(inputs=inp, outputs=out, name=name)\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
    "              loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "# 1D-CNN (slightly deeper)\n",
    "def build_cnn(name=\"CNN1D\"):\n",
    "    inp = layers.Input(shape=(Xa_tr.shape[1], 1))\n",
    "    x = layers.Conv1D(128, kernel_size=7, activation=\"relu\", padding=\"same\")(inp)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(128, kernel_size=5, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(64, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    m = models.Model(inputs=inp, outputs=out, name=name)\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
    "              loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "# Hybrid: Bidirectional LSTM + GRU\n",
    "def build_hybrid(name=\"Hybrid_BiLSTM_GRU\"):\n",
    "    inp = layers.Input(shape=(Xa_tr.shape[1], 1))\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.15))(inp)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.GRU(64, dropout=0.3, recurrent_dropout=0.15)(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.45)(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    m = models.Model(inputs=inp, outputs=out, name=name)\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "# build models\n",
    "lstm_opt = build_lstm(\"LSTM_OPT\")\n",
    "gru_opt  = build_gru(\"GRU_OPT\")\n",
    "cnn_opt  = build_cnn(\"CNN1D_OPT\")\n",
    "hybrid   = build_hybrid(\"HYBRID_OPT\")\n",
    "\n",
    "# ----- 5) Train models (with class_weight) -----\n",
    "print(\"\\n[Train] LSTM (Optimized)\")\n",
    "lstm_opt.fit(Xa_tr_dl, ya_tr, epochs=EPOCHS_DL2, batch_size=BATCH2//1,\n",
    "             validation_data=(Xa_te_dl, ya_te), callbacks=make_cbs(\"LSTM_OPT\"), class_weight=class_weights, verbose=1)\n",
    "y_pred_lstm = np.argmax(lstm_opt.predict(Xa_te_dl, batch_size=BATCH2, verbose=0), axis=1)\n",
    "print(\"LSTM (Optimized) — Accuracy:\", accuracy_score(ya_te, y_pred_lstm))\n",
    "print(classification_report(ya_te, y_pred_lstm, target_names=le_attack.classes_))\n",
    "save_model_h5_any(lstm_opt, \"LSTM_Multiclass_Phase2_OPT\", extra={\"classes\": list(le_attack.classes_)})\n",
    "show_cm_and_valacc_multiclass(\"LSTM (Multiclass AttackType) — OPT\", ya_te, y_pred_lstm, labels=le_attack.classes_,\n",
    "                              savepath=str(DIR_PLOTS / f\"LSTM_Multiclass_Phase2_OPT-{_stamp()}.png\"))\n",
    "\n",
    "print(\"\\n[Train] GRU (Optimized)\")\n",
    "gru_opt.fit(Xa_tr_dl, ya_tr, epochs=EPOCHS_DL2, batch_size=BATCH2//1,\n",
    "            validation_data=(Xa_te_dl, ya_te), callbacks=make_cbs(\"GRU_OPT\"), class_weight=class_weights, verbose=1)\n",
    "y_pred_gru = np.argmax(gru_opt.predict(Xa_te_dl, batch_size=BATCH2, verbose=0), axis=1)\n",
    "print(\"GRU (Optimized) — Accuracy:\", accuracy_score(ya_te, y_pred_gru))\n",
    "print(classification_report(ya_te, y_pred_gru, target_names=le_attack.classes_))\n",
    "save_model_h5_any(gru_opt, \"GRU_Multiclass_Phase2_OPT\", extra={\"classes\": list(le_attack.classes_)})\n",
    "show_cm_and_valacc_multiclass(\"GRU (Multiclass AttackType) — OPT\", ya_te, y_pred_gru, labels=le_attack.classes_,\n",
    "                              savepath=str(DIR_PLOTS / f\"GRU_Multiclass_Phase2_OPT-{_stamp()}.png\"))\n",
    "\n",
    "print(\"\\n[Train] 1D-CNN (Optimized)\")\n",
    "cnn_opt.fit(Xa_tr_dl, ya_tr, epochs=EPOCHS_DL2, batch_size=BATCH2//1,\n",
    "            validation_data=(Xa_te_dl, ya_te), callbacks=make_cbs(\"CNN1D_OPT\"), class_weight=class_weights, verbose=1)\n",
    "y_pred_cnn = np.argmax(cnn_opt.predict(Xa_te_dl, batch_size=BATCH2, verbose=0), axis=1)\n",
    "print(\"1D-CNN (Optimized) — Accuracy:\", accuracy_score(ya_te, y_pred_cnn))\n",
    "print(classification_report(ya_te, y_pred_cnn, target_names=le_attack.classes_))\n",
    "save_model_h5_any(cnn_opt, \"CNN1D_Multiclass_Phase2_OPT\", extra={\"classes\": list(le_attack.classes_)})\n",
    "show_cm_and_valacc_multiclass(\"1D-CNN (Multiclass AttackType) — OPT\", ya_te, y_pred_cnn, labels=le_attack.classes_,\n",
    "                              savepath=str(DIR_PLOTS / f\"CNN1D_Multiclass_Phase2_OPT-{_stamp()}.png\"))\n",
    "\n",
    "print(\"\\n[Train] Hybrid BiLSTM+GRU (Optimized)\")\n",
    "hybrid.fit(Xa_tr_dl, ya_tr, epochs=EPOCHS_DL2, batch_size=BATCH2//1,\n",
    "           validation_data=(Xa_te_dl, ya_te), callbacks=make_cbs(\"HYBRID_OPT\"), class_weight=class_weights, verbose=1)\n",
    "y_pred_hybrid = np.argmax(hybrid.predict(Xa_te_dl, batch_size=BATCH2, verbose=0), axis=1)\n",
    "print(\"Hybrid (Optimized) — Accuracy:\", accuracy_score(ya_te, y_pred_hybrid))\n",
    "print(classification_report(ya_te, y_pred_hybrid, target_names=le_attack.classes_))\n",
    "save_model_h5_any(hybrid, \"HYBRID_Multiclass_Phase2_OPT\", extra={\"classes\": list(le_attack.classes_)})\n",
    "show_cm_and_valacc_multiclass(\"Hybrid (Multiclass AttackType) — OPT\", ya_te, y_pred_hybrid, labels=le_attack.classes_,\n",
    "                              savepath=str(DIR_PLOTS / f\"HYBRID_Multiclass_Phase2_OPT-{_stamp()}.png\"))\n",
    "\n",
    "# ----- 6) Ensemble: soft voting (average predicted probabilities) -----\n",
    "print(\"\\n[ENSEMBLE] Soft voting of LSTM + GRU + CNN + Hybrid\")\n",
    "p_lstm = lstm_opt.predict(Xa_te_dl, batch_size=BATCH2, verbose=0)    # shape (N, num_classes)\n",
    "p_gru  = gru_opt.predict(Xa_te_dl, batch_size=BATCH2, verbose=0)\n",
    "p_cnn  = cnn_opt.predict(Xa_te_dl, batch_size=BATCH2, verbose=0)\n",
    "p_hyb  = hybrid.predict(Xa_te_dl, batch_size=BATCH2, verbose=0)\n",
    "\n",
    "# equal weights (you can adjust weights if you want)\n",
    "p_avg = (p_lstm + p_gru + p_cnn + p_hyb) / 4.0\n",
    "y_pred_ens = np.argmax(p_avg, axis=1)\n",
    "\n",
    "print(\"Ensemble (soft voting) — Accuracy:\", accuracy_score(ya_te, y_pred_ens))\n",
    "print(classification_report(ya_te, y_pred_ens, target_names=le_attack.classes_))\n",
    "show_cm_and_valacc_multiclass(\"Ensemble (LSTM+GRU+CNN+Hybrid) — OPT\", ya_te, y_pred_ens, labels=le_attack.classes_,\n",
    "                              savepath=str(DIR_PLOTS / f\"Ensemble_Multiclass_Phase2_OPT-{_stamp()}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcdfe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% DCN v2 — Phase-2 (Multiclass AttackType)\n",
    "# Yêu cầu sẵn có: Xa_tr, Xa_te, ya_tr, ya_te, le_attack, DIR_PLOTS, _stamp,\n",
    "# save_model_h5_any, show_cm_and_valacc_multiclass, feature_candidates\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks # type: ignore\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def DCN_v2_block(x, depth=4, name=\"dcn_mc\"):\n",
    "    \"\"\"Cross Network v2: x_{l+1} = x0 * (W_l x_l) + b_l + x_l\"\"\"\n",
    "    x0 = x\n",
    "    for i in range(depth):\n",
    "        xl = layers.Dense(x.shape[-1], use_bias=True, name=f\"{name}_w{i+1}\")(x)\n",
    "        x  = layers.Add(name=f\"{name}_add{i+1}\")([x0 * xl, x])\n",
    "    return x\n",
    "\n",
    "num_classes = len(le_attack.classes_)\n",
    "\n",
    "inp = layers.Input(shape=(Xa_tr.shape[1],))\n",
    "# Cross (wide)\n",
    "cross = DCN_v2_block(inp, depth=4, name=\"dcn_mc\")\n",
    "# Deep\n",
    "deep = layers.Dense(256, activation=\"relu\")(inp)\n",
    "deep = layers.Dropout(0.2)(deep)\n",
    "deep = layers.Dense(128, activation=\"relu\")(deep)\n",
    "deep = layers.Dropout(0.2)(deep)\n",
    "deep = layers.Dense(64, activation=\"relu\")(deep)\n",
    "# Kết hợp\n",
    "h = layers.Concatenate()([cross, deep])\n",
    "h = layers.Dense(128, activation=\"relu\")(h)\n",
    "out = layers.Dense(num_classes, activation=\"softmax\")(h)\n",
    "\n",
    "dcn_mc = models.Model(inp, out, name=\"DCNv2_Multiclass\")\n",
    "dcn_mc.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "               loss=\"sparse_categorical_crossentropy\",\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "cb2 = [\n",
    "    callbacks.EarlyStopping(patience=4, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
    "    callbacks.ReduceLROnPlateau(patience=2, factor=0.5, min_lr=1e-5),\n",
    "]\n",
    "\n",
    "dcn_mc.fit(Xa_tr, ya_tr, epochs=30, batch_size=2048,\n",
    "           validation_data=(Xa_te, ya_te), callbacks=cb2, verbose=1)\n",
    "\n",
    "ya_pred_dcn = tf.argmax(dcn_mc.predict(Xa_te, batch_size=4096, verbose=0), axis=1).numpy()\n",
    "\n",
    "print(\"\\nDCN v2 (Multiclass) — Accuracy:\", accuracy_score(ya_te, ya_pred_dcn))\n",
    "print(classification_report(ya_te, ya_pred_dcn, target_names=le_attack.classes_))\n",
    "\n",
    "try:\n",
    "    show_cm_and_valacc_multiclass(\"DCN v2 (Multiclass AttackType)\",\n",
    "                                  ya_te, ya_pred_dcn, labels=le_attack.classes_,\n",
    "                                  savepath=str(DIR_PLOTS / f\"DCNv2_Multiclass_Phase2-{_stamp()}.png\"))\n",
    "except Exception:\n",
    "    show_cm_and_valacc_multiclass(\"DCN v2 (Multiclass AttackType)\",\n",
    "                                  ya_te, ya_pred_dcn, labels=le_attack.classes_)\n",
    "\n",
    "save_model_h5_any(dcn_mc, \"DCNv2_Multiclass_Phase2\",\n",
    "                  extra={\"classes\": list(le_attack.classes_),\n",
    "                         \"feature_order\": feature_candidates})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbb6ad4",
   "metadata": {},
   "source": [
    "# **Dọn Ram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9016ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, sys, types, numpy as np, pandas as pd\n",
    "\n",
    "KEEP = {\n",
    "    # artifacts cần giữ\n",
    "    \"feature_candidates\", \"scaler\", \"le_attack\",\n",
    "    \"xgb_bin\",\"lgb_bin\",\"cat_bin\",\"lstm\",\"gru\",\"cnn\",\"ae\",\n",
    "    \"metrics_phase1\",\"metrics_phase2\",\"best_thresholds\",\n",
    "    # config/seed\n",
    "    \"RANDOM_STATE\",\"split_info\"\n",
    "}\n",
    "\n",
    "SIZE_MB_THRESHOLD = 0  # chỉ dọn biến > 100MB để an toàn\n",
    "\n",
    "def nbytes(obj):\n",
    "    try:\n",
    "        if isinstance(obj, np.ndarray): return obj.nbytes\n",
    "        if isinstance(obj, pd.DataFrame): return obj.memory_usage(deep=True).sum()\n",
    "        if isinstance(obj, pd.Series): return obj.memory_usage(deep=True)\n",
    "        return sys.getsizeof(obj)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "deleted = []\n",
    "for name, val in list(globals().items()):\n",
    "    if name.startswith(\"_\") or name in KEEP: \n",
    "        continue\n",
    "    if isinstance(val, types.ModuleType) or isinstance(val, types.FunctionType):\n",
    "        continue\n",
    "    try:\n",
    "        mb = nbytes(val) / (1024**2)\n",
    "        if mb >= SIZE_MB_THRESHOLD:\n",
    "            del globals()[name]\n",
    "            deleted.append((name, f\"{mb:.1f} MB\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "gc.collect()\n",
    "print(\"Đã dọn:\", deleted[:10], \"... tổng:\", len(deleted))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
