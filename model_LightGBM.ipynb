{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b61a24",
   "metadata": {},
   "source": [
    "\n",
    "# **Imports & cấu hình chung**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18057ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # IDS Notebook — Pipeline 2-Phase + Save/Plot + Resume\n",
    "# - Lưu mô hình dạng `.h5` vào D:\\DACN\\results\\training\\models\n",
    "# - Ảnh biểu đồ vào D:\\DACN\\results\\training\\plots\n",
    "# - Bảng so sánh vào D:\\DACN\\results\\training\\tables\n",
    "# - Có thể **tiếp tục train** (resume) mà **không phải chạy lại** tiền xử lý.\n",
    "\n",
    "import os, glob, io, time, json, warnings, joblib, random, pickle, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    precision_recall_fscore_support, accuracy_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==== PATHS (chỉnh theo máy bạn) ====\n",
    "CIC2019_DIR = r'E:\\DACN\\dataset\\CICDDoS2019'\n",
    "CIC2017_DIR = r'E:\\DACN\\dataset\\CICDDoS2017'\n",
    "UNSW15_DIR  = r'E:\\DACN\\dataset\\UNSW_NB15'\n",
    "NSLKDD_DIR  = r'E:\\DACN\\dataset\\NSL-KDD'   # có KDDTrain+.txt, KDDTest+.txt\n",
    "\n",
    "# ==== nơi lưu kết quả ====\n",
    "ROOT_SAVE = Path(r\"E:\\DACN\\results\\training\")\n",
    "DIR_MODELS = ROOT_SAVE / \"models\"\n",
    "DIR_PLOTS  = ROOT_SAVE / \"plots\"\n",
    "DIR_TABLES = ROOT_SAVE / \"tables\"\n",
    "for p in [DIR_MODELS, DIR_PLOTS, DIR_TABLES]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==== loại cột ID/time ====\n",
    "EXCLUDE_ID_COLUMNS = True\n",
    "ID_LIKE_COLS = set([\n",
    "    'Flow ID','FlowID','Timestamp','StartTime','Start Time','stime','time','Date','datetime',\n",
    "    'Src IP','Dst IP','Source IP','Destination IP',\n",
    "    'srcip','dstip','srcip_addr','dstip_addr', \n",
    "    'Src Port','Dst Port','Sport','Dport','srcport','dstport',\n",
    "    'ProtocolName','ProtoName','Service','service','state','attack_cat','label',\n",
    "    'Unnamed: 0','id','No.','Index'\n",
    "])\n",
    "LABEL_CANDS = [\"Label\",\"label\",\"Attack\",\"attack\",\"attack_cat\",\"class\",\"Class\",\"target\",\"category\",\"Category\",\"result\"]\n",
    "\n",
    "# kiểm soát lệch phân bố từ UNSW (toàn attack)\n",
    "MAX_UNSW_RATIO = 0.30   # tối đa 30%\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d557bf",
   "metadata": {},
   "source": [
    "\n",
    "# **Giới hạn CPU + TensorFlow threads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4189767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CPU] total=28 | allow=25 threads (~90%)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import multiprocessing as mp\n",
    "\n",
    "def limit_cpu(fraction: float = 0.90) -> int:\n",
    "    \"\"\"\n",
    "    Giới hạn tài nguyên CPU ~fraction (theo số luồng).\n",
    "    Trả về số threads cho n_jobs/num_threads.\n",
    "    \"\"\"\n",
    "    fraction = max(0.1, min(1.0, float(fraction)))\n",
    "    total = os.cpu_count() or mp.cpu_count() or 1\n",
    "    allow = max(1, math.floor(total * fraction))\n",
    "\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(allow)\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(allow)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(allow)\n",
    "    os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(allow)\n",
    "    os.environ[\"NUMEXPR_MAX_THREADS\"] = str(allow)\n",
    "    os.environ[\"NUMEXPR_NUM_THREADS\"] = str(allow)\n",
    "\n",
    "    try:\n",
    "        from threadpoolctl import threadpool_limits\n",
    "        threadpool_limits(allow)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import psutil\n",
    "        p = psutil.Process()\n",
    "        cpus = list(range(allow))\n",
    "        p.cpu_affinity(cpus)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"[CPU] total={total} | allow={allow} threads (~{fraction*100:.0f}%)\")\n",
    "    return allow\n",
    "\n",
    "threads_allowed = limit_cpu(0.90)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.threading.set_intra_op_parallelism_threads(threads_allowed)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(max(1, threads_allowed//2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734c362",
   "metadata": {},
   "source": [
    "\n",
    "# **Hàm I/O an toàn + Chuẩn hoá nhãn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3749d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_read_any(path: str, nrows=None) -> pd.DataFrame:\n",
    "    low = path.lower()\n",
    "    try:\n",
    "        if low.endswith(\".parquet\"):\n",
    "            return pd.read_parquet(path) if nrows is None else pd.read_parquet(path).head(nrows)\n",
    "        # NSL-KDD .txt không header\n",
    "        if low.endswith(\".txt\") and (\"kddtrain\" in low or \"kddtest\" in low):\n",
    "            df = pd.read_csv(path, header=None)\n",
    "            if df.shape[1] == 43:\n",
    "                cols = [f\"feat_{i}\" for i in range(41)] + [\"label\",\"difficulty\"]\n",
    "            elif df.shape[1] == 42:\n",
    "                cols = [f\"feat_{i}\" for i in range(41)] + [\"label\"]\n",
    "            else:\n",
    "                cols = [f\"col_{i}\" for i in range(df.shape[1])]\n",
    "            df.columns = cols\n",
    "            return df if nrows is None else df.head(nrows)\n",
    "        # CSV chung\n",
    "        for enc in (\"utf-8-sig\",\"utf-8\",\"cp1252\",\"latin1\"):\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc, compression=\"infer\", low_memory=False, nrows=nrows)\n",
    "            except Exception:\n",
    "                continue\n",
    "        return pd.read_csv(path, compression=\"infer\", low_memory=False, nrows=nrows)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] skip {os.path.basename(path)}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def find_label_col(df: pd.DataFrame):\n",
    "    for c in LABEL_CANDS:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "attack_group_map = {\n",
    "    'DrDoS_DNS':'DrDoS','DrDoS_SNMP':'DrDoS','DrDoS_NTP':'DrDoS','DrDoS_MSSQL':'DrDoS',\n",
    "    'DrDoS_SSDP':'DrDoS','DrDoS_UDP':'DrDoS','TFTP':'TFTP',\n",
    "    'UDP':'UDP','UDPLag':'UDP','Syn':'Syn','MSSQL':'MSSQL','LDAP':'LDAP',\n",
    "    'DoS slowloris':'DoS','DoS Slowhttptest':'DoS','DoS Hulk':'DoS','DoS GoldenEye':'DoS',\n",
    "    'Heartbleed':'Other',\n",
    "    'Web Attack � Brute Force':'Web Attack','Web Attack � XSS':'Web Attack','Web Attack � Sql Injection':'Web Attack',\n",
    "    'FTP-Patator':'Brute Force','SSH-Patator':'Brute Force','Infiltration':'Other','Bot':'Other',\n",
    "    'PortScan':'PortScan','NetBIOS':'Other'\n",
    "}\n",
    "\n",
    "def normalize_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    lbl = find_label_col(df)\n",
    "    if lbl is None:\n",
    "        return pd.DataFrame()\n",
    "    df.rename(columns={lbl: \"Label\"}, inplace=True)\n",
    "    df[\"Label\"] = df[\"Label\"].astype(str).str.strip()\n",
    "    df.loc[df[\"Label\"].str.lower().isin([\"normal\",\"benign\",\"non-attack\",\"good\"]), \"Label\"] = \"Benign\"\n",
    "    if \"AttackType\" not in df.columns:\n",
    "        df[\"AttackType\"] = df[\"Label\"]\n",
    "    def group_attack_type(x):\n",
    "        if pd.isna(x): return 'Other'\n",
    "        if x == 'Benign': return 'Benign'\n",
    "        return attack_group_map.get(str(x), 'Other')\n",
    "    df[\"AttackType\"] = df[\"AttackType\"].apply(group_attack_type)\n",
    "    df[\"Label\"] = df[\"Label\"].apply(lambda v: 'Benign' if str(v)=='Benign' else 'DDoS')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b4acd",
   "metadata": {},
   "source": [
    "\n",
    "# **Liệt kê file & union features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471d6fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIC19 train: 7 CIC19 test: 10\n",
      "CIC17: 8 UNSW: 6 NSL: 4\n",
      "Tổng số cột numeric union: 156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['E:\\\\DACN\\\\results\\\\training\\\\feature_order_union.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CIC-2019 train/test\n",
    "cic19_train, cic19_test = [], []\n",
    "for root,_,files in os.walk(CIC2019_DIR):\n",
    "    for fn in files:\n",
    "        if fn.endswith(\"-training.parquet\"): cic19_train.append(os.path.join(root, fn))\n",
    "        if fn.endswith(\"-testing.parquet\"):  cic19_test.append(os.path.join(root, fn))\n",
    "\n",
    "# CIC-2017 parquet\n",
    "cic17_files = glob.glob(os.path.join(CIC2017_DIR, \"**\", \"*.parquet\"), recursive=True)\n",
    "\n",
    "# UNSW: bỏ *_features.csv, *_LIST_EVENTS.csv, *_GT.csv\n",
    "unsw_all = glob.glob(os.path.join(UNSW15_DIR, \"**\", \"*.csv\"), recursive=True)\n",
    "unsw_files = [p for p in unsw_all if (\"features\" not in os.path.basename(p).lower()\n",
    "                                      and \"list_events\" not in os.path.basename(p).lower()\n",
    "                                      and not os.path.basename(p).lower().endswith(\"_gt.csv\"))]\n",
    "\n",
    "# NSL: chỉ .txt\n",
    "nsl_all = glob.glob(os.path.join(NSLKDD_DIR, \"**\", \"*.txt\"), recursive=True)\n",
    "nsl_files = [p for p in nsl_all if (\"kddtrain\" in os.path.basename(p).lower() or\n",
    "                                    \"kddtest\" in os.path.basename(p).lower())]\n",
    "\n",
    "print(\"CIC19 train:\", len(cic19_train), \"CIC19 test:\", len(cic19_test))\n",
    "print(\"CIC17:\", len(cic17_files), \"UNSW:\", len(unsw_files), \"NSL:\", len(nsl_files))\n",
    "\n",
    "def infer_numeric_cols(files: List[str]) -> set:\n",
    "    s = set()\n",
    "    for p in files[:10]:\n",
    "        head = safe_read_any(p, nrows=200)\n",
    "        if head.empty: continue\n",
    "        head = normalize_labels(head)\n",
    "        if head.empty: continue\n",
    "        cols = [c for c in head.columns if c not in ID_LIKE_COLS and c not in (\"Label\",\"AttackType\")]\n",
    "        for c in cols:\n",
    "            if pd.api.types.is_numeric_dtype(head[c]):\n",
    "                s.add(c)\n",
    "    s.add(\"dataset_id\")\n",
    "    return s\n",
    "\n",
    "union_cols = set()\n",
    "union_cols |= infer_numeric_cols(cic19_train + cic19_test)\n",
    "union_cols |= infer_numeric_cols(cic17_files)\n",
    "union_cols |= infer_numeric_cols(unsw_files)\n",
    "union_cols |= infer_numeric_cols(nsl_files)\n",
    "\n",
    "FEATURES = sorted(list(union_cols))\n",
    "print(\"Tổng số cột numeric union:\", len(FEATURES))\n",
    "joblib.dump({\"feature_order\": FEATURES}, ROOT_SAVE / \"feature_order_union.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06df568",
   "metadata": {},
   "source": [
    "\n",
    "# **Load & Normalize datasets + Gộp + Lưu parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947f0b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load ds1: 100%|██████████| 8/8 [00:01<00:00,  4.18it/s]\n",
      "Load ds2: 100%|██████████| 7/7 [00:00<00:00, 41.07it/s]\n",
      "Load ds2: 100%|██████████| 10/10 [00:00<00:00, 28.14it/s]\n",
      "Load ds3: 100%|██████████| 6/6 [00:09<00:00,  1.51s/it]\n",
      "Load ds4: 100%|██████████| 4/4 [00:00<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: {'CIC17': (2313810, 80), 'CIC19_train': (125170, 80), 'UNSW': (257673, 47), 'NSL': (185559, 45)}\n",
      "Đã lưu: E:\\DACN\\results\\training\\df_all_union.parquet\n"
     ]
    }
   ],
   "source": [
    "def load_and_normalize(files: List[str], dataset_id: int) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for p in tqdm(files, desc=f\"Load ds{dataset_id}\"):\n",
    "        df = safe_read_any(p)\n",
    "        if df.empty: \n",
    "            continue\n",
    "        df = normalize_labels(df)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        df[\"dataset_id\"] = dataset_id\n",
    "        out.append(df)\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "df17  = load_and_normalize(cic17_files, 1)\n",
    "df19t = load_and_normalize(cic19_train, 2)\n",
    "df19e = load_and_normalize(cic19_test, 2)\n",
    "dfUN  = load_and_normalize(unsw_files, 3)\n",
    "dfNSL = load_and_normalize(nsl_files, 4)\n",
    "\n",
    "print(\"Shapes:\", {k:v.shape for k,v in {\"CIC17\":df17,\"CIC19_train\":df19t,\"UNSW\":dfUN,\"NSL\":dfNSL}.items()})\n",
    "\n",
    "# gộp chính (2017 + 2019 train)\n",
    "df_main = pd.concat([df17, df19t], ignore_index=True)\n",
    "\n",
    "# hạn chế UNSW (gần như toàn attack)\n",
    "if not dfUN.empty:\n",
    "    cur_ddos = (df_main[\"Label\"]==\"DDoS\").sum()\n",
    "    cap = int(MAX_UNSW_RATIO * max(1, cur_ddos))\n",
    "    dfUN_ddos = dfUN[dfUN[\"Label\"]==\"DDoS\"]\n",
    "    if len(dfUN_ddos) > cap:\n",
    "        dfUN_ddos = dfUN_ddos.sample(cap, random_state=RANDOM_STATE)\n",
    "    dfUN = dfUN_ddos\n",
    "\n",
    "df_all = pd.concat([df_main, dfUN, dfNSL], ignore_index=True)\n",
    "assert not df_all.empty, \"Không có dữ liệu!\"\n",
    "\n",
    "df_all = df_all.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "for c in df_all.columns:\n",
    "    if df_all[c].dtype == \"object\":\n",
    "        df_all[c] = df_all[c].astype(str)\n",
    "\n",
    "parq_path = ROOT_SAVE / \"df_all_union.parquet\"\n",
    "df_all.to_parquet(parq_path, index=False)\n",
    "print(\"Đã lưu:\", parq_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692909f7",
   "metadata": {},
   "source": [
    "\n",
    "# **Đọc lại parquet + Tạo tập train/test + SMOTE + Chuẩn bị DL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e43a354e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đọc lại: (2749109, 166)\n",
      "Số cột dùng: 156\n",
      "Train: (2199287, 156) Test: (549822, 156)\n",
      "After SMOTE: (3386240, 156) | pos_ratio: 0.5\n",
      "DL shapes: (3386240, 156, 1) (549822, 156, 1)\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_parquet(ROOT_SAVE / \"df_all_union.parquet\")\n",
    "print(\"Đọc lại:\", df_all.shape)\n",
    "\n",
    "drop_cols = {'Label','AttackType'}\n",
    "if EXCLUDE_ID_COLUMNS:\n",
    "    drop_cols |= {c for c in df_all.columns if c in ID_LIKE_COLS}\n",
    "feature_candidates = [c for c in FEATURES if c not in drop_cols and c in df_all.columns]\n",
    "print(\"Số cột dùng:\", len(feature_candidates))\n",
    "\n",
    "X = df_all.reindex(columns=feature_candidates, fill_value=0.0).astype(np.float32)\n",
    "y_bin = (df_all['Label'] != 'Benign').astype(int).values\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y_bin, test_size=0.2, random_state=RANDOM_STATE, stratify=y_bin\n",
    ")\n",
    "print(\"Train:\", X_train_raw.shape, \"Test:\", X_test_raw.shape)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_raw.values)\n",
    "joblib.dump(scaler, ROOT_SAVE / 'scaler_union.pkl')\n",
    "\n",
    "X_train_s = scaler.transform(X_train_raw.values)\n",
    "X_test_s  = scaler.transform(X_test_raw.values)\n",
    "\n",
    "sm = SMOTE(random_state=RANDOM_STATE)\n",
    "X_res, y_res = sm.fit_resample(X_train_s, y_train)\n",
    "print(\"After SMOTE:\", X_res.shape, \"| pos_ratio:\", y_res.mean().round(4))\n",
    "\n",
    "# Dữ liệu cho DL (mỗi feature là 1 “time step”)\n",
    "X_train_dl = X_res.astype(np.float32).reshape(-1, X_res.shape[1], 1)\n",
    "X_test_dl  = X_test_s.astype(np.float32).reshape(-1, X_test_s.shape[1], 1)\n",
    "y_train_dl = y_res.astype(np.int32)\n",
    "y_test_dl  = y_test.astype(np.int32)\n",
    "\n",
    "print(\"DL shapes:\", X_train_dl.shape, X_test_dl.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d36b79f",
   "metadata": {},
   "source": [
    "\n",
    "# **Tiện ích Save/Load .h5 + Plot (CM + Val)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aec3b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def _stamp():\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def save_fig_current(fig, name: str):\n",
    "    png = DIR_PLOTS / f\"{name}-{_stamp()}.png\"\n",
    "    fig.savefig(png, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"[SAVE] Figure -> {png}\")\n",
    "    return str(png)\n",
    "\n",
    "def save_model_h5_any(model, name: str, extra: dict | None = None):\n",
    "    \"\"\"\n",
    "    Lưu mô hình dạng .h5:\n",
    "    - Keras: model.save(.h5)\n",
    "    - Sklearn/XGBoost/LightGBM: pickle vào HDF5['pickle'] + attrs['extra_json']\n",
    "    \"\"\"\n",
    "    path = DIR_MODELS / f\"{name}-{_stamp()}.h5\"\n",
    "    # Thử Keras trước\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        if hasattr(model, \"save\") and isinstance(getattr(model, \"save\"), type(tf.keras.Model.save)):\n",
    "            model.save(path)\n",
    "            print(f\"[SAVE] Keras model -> {path}\")\n",
    "            if extra:\n",
    "                with open(str(path).replace(\".h5\", \".meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(extra, f, ensure_ascii=False, indent=2, default=str)\n",
    "            return str(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Non-Keras\n",
    "    blob = pickle.dumps(model, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with h5py.File(path, \"w\") as h5:\n",
    "        h5.create_dataset(\"pickle\", data=np.void(blob))\n",
    "        if extra:\n",
    "            try:\n",
    "                h5.attrs[\"extra_json\"] = json.dumps(extra, default=str)\n",
    "            except Exception:\n",
    "                h5.attrs[\"extra_json\"] = \"{}\"\n",
    "    print(f\"[SAVE] Pickled model-in-HDF5 -> {path}\")\n",
    "    return str(path)\n",
    "\n",
    "def load_model_h5_any(path: str):\n",
    "    import tensorflow as tf\n",
    "    try:\n",
    "        return tf.keras.models.load_model(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    with h5py.File(path, \"r\") as h5:\n",
    "        blob = bytes(h5[\"pickle\"][()])\n",
    "    return pickle.loads(blob)\n",
    "\n",
    "# --- Plot: Binary\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "def show_cm_and_valacc(model_name, y_true, y_prob, threshold=0.5, savepath=None):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    cm  = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "    im = axes[0].imshow(cm, cmap=\"Blues\")\n",
    "    for (i,j),v in np.ndenumerate(cm):\n",
    "        axes[0].text(j, i, str(v), ha=\"center\", va=\"center\", fontsize=10)\n",
    "    axes[0].set_xticks([0,1]); axes[0].set_xticklabels([\"Benign\",\"DDoS\"])\n",
    "    axes[0].set_yticks([0,1]); axes[0].set_yticklabels([\"Benign\",\"DDoS\"])\n",
    "    axes[0].set_xlabel(\"Predicted\"); axes[0].set_ylabel(\"Actual\")\n",
    "    axes[0].set_title(f\"{model_name} — Confusion Matrix\")\n",
    "    fig.colorbar(im, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    axes[1].bar([0], [acc], width=0.5)\n",
    "    axes[1].set_ylim(0, 1.0)\n",
    "    axes[1].set_xticks([0]); axes[1].set_xticklabels([\"Accuracy\"])\n",
    "    axes[1].set_ylabel(\"Value\")\n",
    "    axes[1].set_title(f\"{model_name} — Validation Accuracy\")\n",
    "    axes[1].text(0, min(acc+0.03, 0.98), f\"{acc:.6f}\", ha=\"center\", va=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "    plt.suptitle(f\"{model_name}  |  ACC={acc:.6f}  AUC={auc:.6f}  thr={threshold}\", y=1.04, fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    if savepath: plt.savefig(savepath, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Plot: Multiclass\n",
    "def show_cm_and_valacc_multiclass(model_name, y_true, y_pred, labels, savepath=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(labels)))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    im = axes[0].imshow(cm_norm, cmap=\"Blues\", vmin=0, vmax=1)\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        axes[0].text(j, i, str(v), ha=\"center\", va=\"center\", fontsize=8,\n",
    "                     color=\"white\" if cm_norm[i, j] > 0.5 else \"black\")\n",
    "    axes[0].set_xticks(np.arange(len(labels))); axes[0].set_xticklabels(labels, rotation=90)\n",
    "    axes[0].set_yticks(np.arange(len(labels))); axes[0].set_yticklabels(labels)\n",
    "    axes[0].set_xlabel(\"Predicted\"); axes[0].set_ylabel(\"Actual\")\n",
    "    axes[0].set_title(f\"{model_name} — Confusion Matrix (Normalized)\")\n",
    "    fig.colorbar(im, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    axes[1].bar([0], [acc], width=0.5)\n",
    "    axes[1].set_ylim(0, 1.0)\n",
    "    axes[1].set_xticks([0]); axes[1].set_xticklabels([\"Accuracy\"])\n",
    "    axes[1].set_ylabel(\"Value\")\n",
    "    axes[1].set_title(f\"{model_name} — Validation Accuracy\")\n",
    "    axes[1].text(0, min(acc+0.03, 0.98), f\"{acc:.4f}\", ha=\"center\", va=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "    plt.suptitle(f\"{model_name}  |  ACC={acc:.4f}\", y=1.03, fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"[Saved] {savepath}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7d03c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EarlyStop theo \"cải thiện quá nhỏ\" giữa 2 epoch liên tiếp (Keras) =====\n",
    "from tensorflow.keras import layers, models, callbacks  # type: ignore\n",
    "\n",
    "class StopOnTinyChange(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Dừng sớm nếu metric theo dõi (monitor) cải thiện quá nhỏ giữa 2 epoch liên tiếp.\n",
    "    - monitor: 'val_auc' cho bài toán binary, hoặc 'val_accuracy' cho multiclass.\n",
    "    - min_delta: ngưỡng cải thiện tối thiểu. Nếu |Δ| < min_delta => dừng.\n",
    "    \"\"\"\n",
    "    def __init__(self, monitor=\"val_auc\", min_delta=1e-4):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.min_delta = float(min_delta)\n",
    "        self.prev = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        curr = logs.get(self.monitor)\n",
    "        if curr is None:\n",
    "            return\n",
    "        if self.prev is not None and abs(curr - self.prev) < self.min_delta:\n",
    "            print(f\"\\n[STOP] Δ{self.monitor}={curr - self.prev:.6f} < {self.min_delta} tại epoch {epoch+1}. Kết thúc huấn luyện.\")\n",
    "            self.model.stop_training = True\n",
    "        self.prev = curr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46eea880",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_BIN = []  # lưu so sánh phase-1\n",
    "\n",
    "def eval_binary(y_true, y_prob, name, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    RESULTS_BIN.append({\"Model\": name, \"ACC\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1, \"AUC\": auc})\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Benign\",\"DDoS\"]))\n",
    "    print(\"ROC-AUC:\", auc)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a880750",
   "metadata": {},
   "source": [
    "\n",
    "# **Phase 1: LightGBM (Binary) — Train/Resume + Lưu + Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Phase-1: LightGBM (Binary) — giữ nguyên logic (warm_start theo epoch) ===\n",
    "EPOCHS = 3\n",
    "ROUND_PER_EPOCH = 500\n",
    "\n",
    "clf_bin = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=128,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    class_weight='balanced',\n",
    "    n_estimators=ROUND_PER_EPOCH,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=-1,\n",
    "    warm_start=True\n",
    ")\n",
    "\n",
    "# --- Resume tùy chọn ---\n",
    "# Nếu bạn đã có file .h5 trước đó: đặt path vào đây để tiếp tục tăng n_estimators\n",
    "# ví dụ: r\"D:\\DACN\\results\\training\\models\\LGBM_Binary_Phase1-20251007-120000.h5\"\n",
    "RESUME_LGBM_BIN = r\"E:\\DACN\\results\\training\\models\\LGBM_Binary_Phase1-20251007-225015.h5\"\n",
    "if RESUME_LGBM_BIN and os.path.exists(RESUME_LGBM_BIN):\n",
    "    clf_prev = load_model_h5_any(RESUME_LGBM_BIN)\n",
    "    if isinstance(clf_prev, lgb.LGBMClassifier):\n",
    "        clf_bin = clf_prev\n",
    "        clf_bin.set_params(warm_start=True)\n",
    "        print(\"[RESUME] Loaded previous LightGBM Binary model, will continue with more trees.\")\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    clf_bin.n_estimators = getattr(clf_bin, \"n_estimators\", ROUND_PER_EPOCH) + ROUND_PER_EPOCH\n",
    "    clf_bin.fit(\n",
    "        X_res, y_res,\n",
    "        eval_set=[(X_test_s, y_test)],\n",
    "        eval_metric=['auc','binary_logloss'],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
    "    )\n",
    "    print(f\"[✓] Epoch {e+1}/{EPOCHS} — best_iter: {clf_bin.best_iteration_}\")\n",
    "\n",
    "# Save\n",
    "save_model_h5_any(\n",
    "    clf_bin,\n",
    "    name=\"LGBM_Binary_Phase1\",\n",
    "    extra={\"feature_order\": feature_candidates}\n",
    ")\n",
    "\n",
    "# Plot + Eval\n",
    "yprob_lgb = clf_bin.predict_proba(X_test_s)[:,1]\n",
    "show_cm_and_valacc(\n",
    "    \"LightGBM (Binary)\", y_test, yprob_lgb, threshold=0.5,\n",
    "    savepath=str(DIR_PLOTS / f\"LGBM_Binary_Phase1-{_stamp()}.png\")\n",
    ")\n",
    "_ = eval_binary(y_test, yprob_lgb, \"LightGBM (Binary)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81be7f",
   "metadata": {},
   "source": [
    "\n",
    "# **Phase 2: Chuẩn bị dữ liệu đa lớp (AttackType)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy tất cả mẫu DDoS\n",
    "df_attack = df_all[df_all['Label']=='DDoS'].copy()\n",
    "X_attack = df_attack.reindex(columns=feature_candidates, fill_value=0.0).astype(np.float32)\n",
    "X_attack_s = scaler.transform(X_attack)\n",
    "\n",
    "# Mã hoá nhãn AttackType\n",
    "y_attack_txt = df_attack['AttackType'].astype(str).values\n",
    "le_attack = LabelEncoder()\n",
    "y_attack = le_attack.fit_transform(y_attack_txt)\n",
    "num_classes = len(le_attack.classes_)\n",
    "joblib.dump(le_attack, ROOT_SAVE / \"attack_label_encoder_union.pkl\")\n",
    "print(\"Classes:\", list(le_attack.classes_))\n",
    "\n",
    "# SMOTE multiclass\n",
    "X_attack_res, y_attack_res = SMOTE(random_state=RANDOM_STATE).fit_resample(X_attack_s, y_attack)\n",
    "\n",
    "# Train/test split\n",
    "Xa_tr, Xa_te, ya_tr, ya_te = train_test_split(\n",
    "    X_attack_res, y_attack_res, test_size=0.2, random_state=RANDOM_STATE, stratify=y_attack_res\n",
    ")\n",
    "Xa_tr = Xa_tr.astype(np.float16)\n",
    "Xa_te = Xa_te.astype(np.float16)\n",
    "print(\"Train:\", Xa_tr.shape, \"Test:\", Xa_te.shape)\n",
    "\n",
    "# Lưu để DL Phase-2 dùng\n",
    "Xa_tr_dl = Xa_tr.astype(np.float32).reshape(-1, Xa_tr.shape[1], 1)\n",
    "Xa_te_dl = Xa_te.astype(np.float32).reshape(-1, Xa_te.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20c36b",
   "metadata": {},
   "source": [
    "\n",
    "# **Phase 2: LightGBM (Multiclass) — Train + Lưu + Plot)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a52489",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_multi = lgb.LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=num_classes,\n",
    "    n_estimators=1600,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=128,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "clf_multi.fit(\n",
    "    Xa_tr, ya_tr,\n",
    "    eval_set=[(Xa_te, ya_te)],\n",
    "    eval_metric=['multi_logloss'],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=80, verbose=False)]\n",
    ")\n",
    "\n",
    "save_model_h5_any(\n",
    "    clf_multi,\n",
    "    name=\"LGBM_Multiclass_Phase2\",\n",
    "    extra={\"feature_order\": feature_candidates, \"classes\": list(le_attack.classes_)}\n",
    ")\n",
    "\n",
    "# Đánh giá trên phần DDoS của test từ Phase-1 (đúng logic gốc)\n",
    "test_mask_ddos = (y_test == 1)\n",
    "X_attack_test = X_test_s[test_mask_ddos]\n",
    "y_attack_test_txt = df_all.iloc[X_test_raw.index][test_mask_ddos]['AttackType'].astype(str).values\n",
    "\n",
    "y_attack_test = []\n",
    "for t in y_attack_test_txt:\n",
    "    if t in le_attack.classes_:\n",
    "        y_attack_test.append(le_attack.transform([t])[0])\n",
    "    else:\n",
    "        if \"Other\" in le_attack.classes_:\n",
    "            y_attack_test.append(le_attack.transform([\"Other\"])[0])\n",
    "        else:\n",
    "            y_attack_test.append(-1)\n",
    "y_attack_test = np.array([v for v in y_attack_test if v >= 0])\n",
    "\n",
    "y_attack_pred = clf_multi.predict(X_attack_test[:len(y_attack_test)])\n",
    "\n",
    "print(classification_report(y_attack_test, y_attack_pred, target_names=le_attack.classes_))\n",
    "show_cm_and_valacc_multiclass(\n",
    "    \"LightGBM (Multiclass AttackType)\",\n",
    "    y_attack_test, y_attack_pred, labels=le_attack.classes_,\n",
    "    savepath=str(DIR_PLOTS / f\"LGBM_Multiclass_Phase2-{_stamp()}.png\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bcd405",
   "metadata": {},
   "source": [
    "# **TEST PHASE 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f83872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Phase-1 Trees — KHÔNG SMOTE: dùng scale_pos_weight / is_unbalance\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "spw = max(1.0, neg / max(1, pos))\n",
    "print(\"scale_pos_weight =\", spw)\n",
    "\n",
    "# LightGBM (Binary)\n",
    "lgb_bin = lgb.LGBMClassifier(\n",
    "    objective='binary', learning_rate=0.05, num_leaves=96,\n",
    "    subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "    n_estimators=1200, n_jobs=-1, random_state=42,\n",
    "    is_unbalance=True  # hoặc scale_pos_weight=spw\n",
    ")\n",
    "lgb_bin.fit(X_train_s, y_train, eval_set=[(X_test_s, y_test)],\n",
    "            eval_metric=['auc','binary_logloss'],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
    "yprob_lgb = lgb_bin.predict_proba(X_test_s)[:,1]\n",
    "_ = eval_binary(y_test, yprob_lgb, \"LightGBM (Binary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ea1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ----- Imports -----\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    f1_score, accuracy_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# %% ----- Validation Functions (Từ cell mới nhất của bạn) -----\n",
    "\n",
    "def _best_threshold_f1(y_true: np.ndarray, y_prob: np.ndarray):\n",
    "    \"\"\"Chọn ngưỡng tối ưu theo F1-score.\"\"\"\n",
    "    p, r, th = precision_recall_curve(y_true, y_prob)\n",
    "    # Đảm bảo p và r có cùng độ dài với f1s bằng cách loại bỏ phần tử cuối của p và r\n",
    "    f1s = 2 * p[:-1] * r[:-1] / np.maximum(p[:-1] + r[:-1], 1e-12)\n",
    "    i = int(np.nanargmax(f1s))\n",
    "    # Đảm bảo th[i], p[i], r[i] là các giá trị float chuẩn của Python\n",
    "    return float(th[i]), float(f1s[i]), float(p[i]), float(r[i])\n",
    "\n",
    "def _show_confusion(y_true, y_pred, title):\n",
    "    \"\"\"Vẽ confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4.0), dpi=140)\n",
    "    im = ax.imshow(cm, interpolation='nearest')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    # Đảm bảo cm.flat là một list có thể lặp qua\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, f\"{v}\", ha='center', va='center')\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return cm\n",
    "\n",
    "def validate_inline(name: str, y_true: np.ndarray, y_prob: np.ndarray):\n",
    "    \"\"\"Hàm đánh giá và vẽ biểu đồ chi tiết.\"\"\"\n",
    "    # --- ROC ---\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    plt.figure(figsize=(4.8, 4.0), dpi=140)\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\")\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    plt.title(f\"ROC — {name}\")\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # --- PR ---\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "    plt.figure(figsize=(4.8, 4.0), dpi=140)\n",
    "    plt.plot(rec, prec, label=f\"AP = {ap:.4f}\")\n",
    "    plt.title(f\"Precision–Recall — {name}\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.legend(loc=\"lower left\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # --- thresholds: 0.5 và best-F1 ---\n",
    "    th_f1, best_f1, best_p, best_r = _best_threshold_f1(y_true, y_prob)\n",
    "    thresholds = [0.5, th_f1]\n",
    "    th_labels = [\"0.50\", f\"F1*={th_f1:.4f}\"]\n",
    "\n",
    "    print(f\"\\n=== {name} — Summary ===\")\n",
    "    print(f\"ROC AUC = {roc_auc:.6f} | AP = {ap:.6f}\")\n",
    "    print(f\"Best-F1 threshold = {th_f1:.6f} | F1={best_f1:.6f} | P={best_p:.6f} | R={best_r:.6f}\")\n",
    "\n",
    "    for th, tag in zip(thresholds, th_labels):\n",
    "        y_pred = (y_prob >= th).astype(int)\n",
    "        acc  = accuracy_score(y_true, y_pred)\n",
    "        f1   = f1_score(y_true, y_pred)\n",
    "        p    = precision_score(y_true, y_pred, zero_division=0)\n",
    "        r    = recall_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"\\n--- Metrics @ th={tag} ---\")\n",
    "        print(f\"Accuracy={acc:.6f} | F1={f1:.6f} | Precision={p:.6f} | Recall={r:.6f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "        _ = _show_confusion(y_true, y_pred, f\"{name} — Confusion (th={tag})\")\n",
    "\n",
    "# %% ----- Training & Evaluation (LightGBM) -----\n",
    "# (Giả sử X_train_s, y_train, X_test_s, y_test đã được định nghĩa ở đâu đó trước đây)\n",
    "\n",
    "# 1. Tính scale_pos_weight (từ cell trước của bạn)\n",
    "# neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "# spw = max(1.0, neg / max(1, pos))\n",
    "# print(\"scale_pos_weight =\", spw)\n",
    "\n",
    "# 2. Huấn luyện LightGBM\n",
    "print(\"Đang huấn luyện LightGBM...\")\n",
    "lgb_bin = lgb.LGBMClassifier(\n",
    "    objective='binary', learning_rate=0.05, num_leaves=96,\n",
    "    subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "    n_estimators=1200, n_jobs=-1, random_state=42,\n",
    "    is_unbalance=True  # Tham số này xử lý mất cân bằng, tương tự scale_pos_weight\n",
    ")\n",
    "lgb_bin.fit(X_train_s, y_train, eval_set=[(X_test_s, y_test)],\n",
    "            eval_metric=['auc','binary_logloss'],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
    "\n",
    "# 3. Lấy xác suất dự đoán\n",
    "yprob_lgb = lgb_bin.predict_proba(X_test_s)[:,1]\n",
    "\n",
    "# 4. Gọi hàm đánh giá chi tiết\n",
    "print(\"Đang đánh giá LightGBM...\")\n",
    "validate_inline(\"LightGBM (Binary)\",  y_test, yprob_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbb6ad4",
   "metadata": {},
   "source": [
    "# **Dọn Ram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9016ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, sys, types, numpy as np, pandas as pd\n",
    "\n",
    "KEEP = {\n",
    "    # artifacts cần giữ\n",
    "    \"feature_candidates\", \"scaler\", \"le_attack\",\n",
    "    \"xgb_bin\",\"lgb_bin\",\"cat_bin\",\"lstm\",\"gru\",\"cnn\",\"ae\",\n",
    "    \"metrics_phase1\",\"metrics_phase2\",\"best_thresholds\",\n",
    "    # config/seed\n",
    "    \"RANDOM_STATE\",\"split_info\"\n",
    "}\n",
    "\n",
    "SIZE_MB_THRESHOLD = 0  # chỉ dọn biến > 100MB để an toàn\n",
    "\n",
    "def nbytes(obj):\n",
    "    try:\n",
    "        if isinstance(obj, np.ndarray): return obj.nbytes\n",
    "        if isinstance(obj, pd.DataFrame): return obj.memory_usage(deep=True).sum()\n",
    "        if isinstance(obj, pd.Series): return obj.memory_usage(deep=True)\n",
    "        return sys.getsizeof(obj)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "deleted = []\n",
    "for name, val in list(globals().items()):\n",
    "    if name.startswith(\"_\") or name in KEEP: \n",
    "        continue\n",
    "    if isinstance(val, types.ModuleType) or isinstance(val, types.FunctionType):\n",
    "        continue\n",
    "    try:\n",
    "        mb = nbytes(val) / (1024**2)\n",
    "        if mb >= SIZE_MB_THRESHOLD:\n",
    "            del globals()[name]\n",
    "            deleted.append((name, f\"{mb:.1f} MB\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "gc.collect()\n",
    "print(\"Đã dọn:\", deleted[:10], \"... tổng:\", len(deleted))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
